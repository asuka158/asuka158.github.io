<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>


    <meta name="description" content="cuda_learning_01       一、GPU 与 CUDA 结构       二、CUDA编程的要素       三、实践：PyTorch自定义CUDA算子">
<meta property="og:type" content="article">
<meta property="og:title" content="cuda_learning_01">
<meta property="og:url" content="http://example.com/2024/11/30/cuda-learning-01/index.html">
<meta property="og:site_name" content="Asuka">
<meta property="og:description" content="cuda_learning_01       一、GPU 与 CUDA 结构       二、CUDA编程的要素       三、实践：PyTorch自定义CUDA算子">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/11/30/cuda-learning-01/image-20241130185858511.png">
<meta property="og:image" content="http://example.com/2024/11/30/cuda-learning-01/image-20241130212501862.png">
<meta property="og:image" content="http://example.com/2024/11/30/cuda-learning-01/image-20241130221814184.png">
<meta property="og:image" content="http://example.com/2024/11/30/cuda-learning-01/image-20241130225455580.png">
<meta property="og:image" content="http://example.com/2024/11/30/cuda-learning-01/image-20241130223355252.png">
<meta property="og:image" content="http://example.com/2024/11/30/cuda-learning-01/image-20241130224033938.png">
<meta property="article:published_time" content="2024-11-30T10:49:07.000Z">
<meta property="article:modified_time" content="2025-01-22T13:54:20.384Z">
<meta property="article:author" content="Asuka">
<meta property="article:tag" content="CUDA编程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/11/30/cuda-learning-01/image-20241130185858511.png">


<link rel="canonical" href="http://example.com/2024/11/30/cuda-learning-01/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/11/30/cuda-learning-01/","path":"2024/11/30/cuda-learning-01/","title":"cuda_learning_01"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>cuda_learning_01 | Asuka</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Asuka</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">
    cuda_learning_01
</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81GPU-%E4%B8%8E-CUDA-%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.</span> <span class="nav-text">一、GPU 与 CUDA 结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CPU-%E4%B8%8E-GPU"><span class="nav-number">1.1.1.</span> <span class="nav-text">CPU 与 GPU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA%E7%BB%93%E6%9E%84"><span class="nav-number">1.1.2.</span> <span class="nav-text">CUDA结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E4%BA%8ESM-%E7%BA%BF%E7%A8%8B%E5%9D%97%EF%BC%8Cshare-memory%EF%BC%8C%E7%BA%BF%E7%A8%8B%E6%9D%9F"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">关于SM, 线程块，share memory，线程束</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81CUDA%E7%BC%96%E7%A8%8B%E7%9A%84%E8%A6%81%E7%B4%A0"><span class="nav-number">1.2.</span> <span class="nav-text">二、CUDA编程的要素</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E8%B7%B5%EF%BC%9APyTorch%E8%87%AA%E5%AE%9A%E4%B9%89CUDA%E7%AE%97%E5%AD%90"><span class="nav-number">1.3.</span> <span class="nav-text">三、实践：PyTorch自定义CUDA算子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#JIT-%E7%BC%96%E8%AF%91%E8%B0%83%E7%94%A8"><span class="nav-number">1.3.1.</span> <span class="nav-text">JIT 编译调用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SETUP-%E7%BC%96%E8%AF%91%E8%B0%83%E7%94%A8"><span class="nav-number">1.3.2.</span> <span class="nav-text">SETUP 编译调用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CMAKE-%E7%BC%96%E8%AF%91%E8%B0%83%E7%94%A8"><span class="nav-number">1.3.3.</span> <span class="nav-text">CMAKE 编译调用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.4.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Asuka"
      src="/images/asuka.jpg">
  <p class="site-author-name" itemprop="name">Asuka</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/11/30/cuda-learning-01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/asuka.jpg">
      <meta itemprop="name" content="Asuka">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Asuka">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="cuda_learning_01 | Asuka">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cuda_learning_01
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-30 18:49:07" itemprop="dateCreated datePublished" datetime="2024-11-30T18:49:07+08:00">2024-11-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-01-22 21:54:20" itemprop="dateModified" datetime="2025-01-22T21:54:20+08:00">2025-01-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/HPC/" itemprop="url" rel="index"><span itemprop="name">HPC</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p></p><h1 style="text-align: center;">
    cuda_learning_01
</h1><p></p>
<div style="text-align: center;">
    <strong>一、GPU 与 CUDA 结构</strong>
</div>
<div style="text-align: center;">
    <strong>二、CUDA编程的要素</strong>
</div>
<div style="text-align: center;">
    <strong>三、实践：PyTorch自定义CUDA算子</strong>
</div>




<span id="more"></span>
<p>&emsp;&emsp;内容主要来自知乎文章，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/645330027">CUDA（一）：CUDA 编程基础</a>，本文是学习笔记。</p>
<h2 id="一、GPU-与-CUDA-结构"><a href="#一、GPU-与-CUDA-结构" class="headerlink" title="一、GPU 与 CUDA 结构"></a>一、GPU 与 CUDA 结构</h2><h3 id="CPU-与-GPU"><a href="#CPU-与-GPU" class="headerlink" title="CPU 与 GPU"></a>CPU 与 GPU</h3><p>&emsp;&emsp;对于处理器而言，有2个指标是最主要的：<strong>延迟</strong>和<strong>吞吐量</strong>。</p>
<p>&emsp;&emsp;下图左是 CPU 的示意图，有以下几个特点：</p>
<ol>
<li><strong>CPU 中包含了多级高速的缓存结构。</strong> 这样提升了指令访问存储的速度。</li>
<li><strong>CPU 中包含了很多控制单元。</strong> 具体有2种，一个是分支预测机制，另一个是流水线前传机制。</li>
<li><strong>CPU 的运算单元 (Core) 强大，整型浮点型复杂运算速度快。</strong></li>
</ol>
<p>&emsp;&emsp;基于以上三点，CPU 在设计时的导向就是减少指令的时延，被称之为<strong>延迟导向设计</strong>。</p>
<p>&emsp;&emsp;下图右是 GPU 的示意图，有以下几个特点：</p>
<ol>
<li><strong>GPU 中虽有缓存结构但是数量少。</strong> 因为要减少指令访问缓存的次数。</li>
<li><strong>GPU 中控制单元非常简单。</strong> 控制单元中没有分支预测机制和数据转发机制，对于复杂的指令运算就会比较慢。</li>
<li><strong>GPU 的运算单元 (Core) 非常多，采用长延时流水线以实现高吞吐量。</strong> 每一行的运算单元的控制器只有一个，意味着每一行的运算单元使用的指令是相同的，不同的是它们的数据内容。那么这种整齐划一的运算方式使得 GPU 对于那些控制简单但运算高效的指令的效率显著增加。</li>
</ol>
<p>&emsp;&emsp;基于此，可以看到 GPU 在设计过程中以一个原则为核心：增加简单指令的吞吐，这称 GPU 为吞吐导向设计。</p>
<p><img src="/2024/11/30/cuda-learning-01/image-20241130185858511.png" alt="image-20241130185858511"></p>
<div align="center" style="color: #aaa;">   GPU vs CPU </div>

<h3 id="CUDA结构"><a href="#CUDA结构" class="headerlink" title="CUDA结构"></a>CUDA结构</h3><p>&emsp;&emsp;从<strong>硬件</strong>的角度来讲，CUDA 内存模型的最基本的<strong>计算单位</strong>是 <strong>SP (线程处理器)</strong>。每个线程处理器 (SP) 都有自己的 <strong>registers (寄存器)</strong> 和 <strong>local memory (局部内存)</strong>。寄存器和局部内存只能被自己访问，不同的线程处理器之间彼此独立。</p>
<p>&emsp;&emsp;多个线程处理器 (SP) 和一块共享内存构成 <strong>SM (多核处理器)</strong> (灰色部分)。多核处理器里边的多个线程处理器是互相并行的，是不互相影响的。每个多核处理器 (SM) 内都有自己的 shared memory (共享内存)，shared memory 可以被线程块内所有线程访问。</p>
<p>&emsp;&emsp;再往上，由这个 SM (多核处理器) 和一块全局内存，就构成了 GPU。一个 GPU 的所有 SM 共有一块 global memory (全局内存)，不同线程块的线程都可使用。</p>
<p>&emsp;&emsp;上面这段话可以表述为：<strong>每个 thread 都有自己的一份 register 和 local memory 的空间。同一个 block 中的每个 thread 则有共享的一份 share memory。此外，所有的 thread (包括不同 block 的 thread) 都共享一份 global memory。不同的 grid 则有各自的 global memory。</strong></p>
<p><img src="/2024/11/30/cuda-learning-01/image-20241130212501862.png" alt="image-20241130212501862"></p>
<div align="center" style="color: #aaa;">   CUDA 内存模型（硬件） </div>

<p>&emsp;&emsp;从<strong>软件</strong>的角度来讲：</p>
<ol>
<li>线程处理器 (SP) 对应线程 (thread)。</li>
<li>多核处理器 (SM) 对应线程块 (thread block)。</li>
<li>设备端 (device) 对应线程块组合体 (grid)。</li>
</ol>
<h4 id="关于SM-线程块，share-memory，线程束"><a href="#关于SM-线程块，share-memory，线程束" class="headerlink" title="关于SM, 线程块，share memory，线程束"></a>关于SM, 线程块，share memory，线程束</h4><p>&emsp;&emsp;<strong>一个 线程块（Thread Block）</strong> 会被分配到<strong>一个 SM（Streaming Multiprocessor）</strong> 上执行。<strong>一个SM</strong>可以同时执行<strong>多个线程块</strong>，前提是该SM有足够的资源来容纳这些线程块（例如寄存器、共享内存等）。</p>
<p>&emsp;&emsp;<strong>每个SM</strong>都有<strong>一块共享内存（Shared Memory）</strong>，这块共享内存是所有在该SM上执行的线程块共享的，但线程块之间不能访问彼此的共享内存。在<strong>一个线程块内</strong>，所有线程共享<strong>该线程块的</strong>共享内存。</p>
<p>&emsp;&emsp;GPU 的<strong>每一行由1个控制单元加上若干计算单元所组成，这些所有的计算单元执行的控制指令是一个</strong>。这是个非常典型的<strong>“单指令多数据流机制(SIMT)”</strong>，这一行称为一个<strong>线程束 (warp)</strong>。</p>
<p>&emsp;&emsp;当<strong>线程块</strong>被划分到某个<strong>SM</strong>上时，它将进一步<strong>划分为多个线程束</strong>，因为<strong>线程束</strong>是SM的<strong>基本执行单元</strong>，但是一个SM同时并发的线程束数是有限的。这是因为资源限制，SM要为每个线程块分配共享内存，而也要为每个线程束中的线程分配独立的寄存器。所以SM的配置会影响其所支持的线程块和线程束并发数量。<strong>总之，就是网格和线程块只是逻辑划分，SM才是执行的物理层，一个kernel的所有线程其实在物理层是不一定同时并发的。</strong>所以kernel的grid和block的配置不同，性能会出现差异，这点是要特别注意的。还有，<strong>由于SM的基本执行单元是包含32个线程的线程束，所以block大小一般要设置为32的倍数。</strong></p>
<p><img src="/2024/11/30/cuda-learning-01/image-20241130221814184.png" alt="image-20241130221814184"></p>
<div align="center" style="color: #aaa;">   CUDA 内存模型（软件） </div>

<p><img src="/2024/11/30/cuda-learning-01/image-20241130225455580.png" alt="image-20241130225455580"></p>
<p>&emsp;&emsp;关于不同设备的硬件情况，手上只有自己的笔记本和实验室高贵的1080。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">	<span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp devProp;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;devProp, dev);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Using GPU device &quot;</span> &lt;&lt; dev &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; devProp.name &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Number of SMs: &quot;</span> &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Shared memory per block: &quot;</span> &lt;&lt; devProp.sharedMemPerBlock / <span class="number">1024.0</span> &lt;&lt; <span class="string">&quot; KB&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Max threads per block: &quot;</span> &lt;&lt; devProp.maxThreadsPerBlock &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Max threads per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Max thread warps per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor / <span class="number">32</span> &lt;&lt; std::endl;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">// 自己的笔记本</span></span><br><span class="line"><span class="comment">使用GPU device 0: NVIDIA GeForce MX350</span></span><br><span class="line"><span class="comment">SM的数量：5</span></span><br><span class="line"><span class="comment">每个线程块的共享内存大小：48 KB       </span></span><br><span class="line"><span class="comment">每个线程块的最大线程数：1024</span></span><br><span class="line"><span class="comment">每个SM的最大线程数：2048</span></span><br><span class="line"><span class="comment">每个SM的最大线程束数：64</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">// 实验室高贵的一代神卡1080ti</span></span><br><span class="line"><span class="comment">Using GPU device 0: GeForce GTX 1080 Ti</span></span><br><span class="line"><span class="comment">Number of SMs: 28</span></span><br><span class="line"><span class="comment">Shared memory per block: 48 KB</span></span><br><span class="line"><span class="comment">Max threads per block: 1024</span></span><br><span class="line"><span class="comment">Max threads per SM: 2048</span></span><br><span class="line"><span class="comment">Max thread warps per SM: 64    </span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<h2 id="二、CUDA编程的要素"><a href="#二、CUDA编程的要素" class="headerlink" title="二、CUDA编程的要素"></a>二、CUDA编程的要素</h2><p>&emsp;&emsp;一直困扰我的线程块的结构，我搞不太清队医一个大小为$(D_x,D_y)$的线程块，<strong>$D_x$</strong>指的是列数还是行数。目前看来应该是<strong>列数</strong>，即下图，图片来源，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/680075822">知乎@离心</a>。</p>
<p><img src="/2024/11/30/cuda-learning-01/image-20241130223355252.png" alt="image-20241130223355252"></p>
<p>&emsp;&emsp;明白了这个后，计算线程ID，就能推导了。</p>
<p><img src="/2024/11/30/cuda-learning-01/image-20241130224033938.png" alt="image-20241130224033938"></p>
<h2 id="三、实践：PyTorch自定义CUDA算子"><a href="#三、实践：PyTorch自定义CUDA算子" class="headerlink" title="三、实践：PyTorch自定义CUDA算子"></a>三、实践：PyTorch自定义CUDA算子</h2><p>&emsp;&emsp;这部分使用 CUDA 定义一个简单的算子（两个矩阵相加），并将其绑定到pytorch上，然后通过<code>run_time.py</code>这个脚本计算执行时间。我们先着重学习绑定这个过程，可以通过有三种方法实现。简单来说就是pytorch不能调cuda的内核函数，需要一个接口，即我们将c++代码与pytorch代码绑定，调用pytorch代码即调用对应的c++代码。这部分案例主要参考<a href="https://link.zhihu.com/?target=https%3A//github.com/godweiyang/NN-CUDA-Example">github.com/godweiyang/NN-CUDA-Example</a>和<a href="https://link.zhihu.com/?target=https%3A//github.com/ifromeast/cuda_learning/tree/main/01_cuda_op">github.com/ifromeast/cuda_learning</a>。</p>
<p>&emsp;&emsp;Torch 使用CUDA 算子 主要分为三个步骤：</p>
<ul>
<li>先编写CUDA算子和对应的调用函数。</li>
<li>然后编写torch cpp函数建立PyTorch和CUDA之间的联系，用pybind11封装。</li>
<li>最后用PyTorch的cpp扩展库进行编译和调用。</li>
</ul>
<p>&emsp;&emsp;代码结构如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">├── include </span><br><span class="line">│   └── add2.h           	# cuda算子的头文件 </span><br><span class="line">├── kernel </span><br><span class="line">│   ├── add2_kernel.cu   	# cuda算子的具体实现 </span><br><span class="line">│   └── add2_ops.cpp        # cuda算子的cpp torch封装 </span><br><span class="line">├── CMakeLists.txt </span><br><span class="line">├── LICENSE </span><br><span class="line">├── README.md </span><br><span class="line">├── setup.py </span><br><span class="line">├── run_time.py             # 比较cuda算子和torch实现的时间差异 </span><br><span class="line">└── train.py 				# 使用cuda算子来训练模型</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;代码实现：（可以当作模板）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// include/add2.h</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_add2</span><span class="params">(<span class="type">float</span> *c,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">const</span> <span class="type">float</span> *a,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">const</span> <span class="type">float</span> *b,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> n)</span></span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// kernel/add2_kernel.cu</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatAdd</span><span class="params">(<span class="type">float</span>* c,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="type">const</span> <span class="type">float</span>* a,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="type">const</span> <span class="type">float</span>* b,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> j = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="type">int</span> idx = j*n + i;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; n &amp;&amp; j &lt; n)</span><br><span class="line">        c[idx] = a[idx] + b[idx];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_add2</span><span class="params">(<span class="type">float</span>* c,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">const</span> <span class="type">float</span>* a,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">const</span> <span class="type">float</span>* b,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(n/block.x, n/block.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    MatAdd&lt;&lt;&lt;grid, block&gt;&gt;&gt;(c, a, b, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// kernel/add2_ops.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;add2.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">torch_launch_add2</span><span class="params">(torch::Tensor &amp;c,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">const</span> torch::Tensor &amp;a,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">const</span> torch::Tensor &amp;b,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">int64_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">launch_add2</span>((<span class="type">float</span> *)c.<span class="built_in">data_ptr</span>(),</span><br><span class="line">                (<span class="type">const</span> <span class="type">float</span> *)a.<span class="built_in">data_ptr</span>(),</span><br><span class="line">                (<span class="type">const</span> <span class="type">float</span> *)b.<span class="built_in">data_ptr</span>(),</span><br><span class="line">                n);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;torch_launch_add2&quot;</span>,</span><br><span class="line">          &amp;torch_launch_add2,</span><br><span class="line">          <span class="string">&quot;add2 kernel warpper&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY</span>(add2, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;torch_launch_add2&quot;</span>, torch_launch_add2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="JIT-编译调用"><a href="#JIT-编译调用" class="headerlink" title="JIT 编译调用"></a>JIT 编译调用</h3><p>&emsp;&emsp;just-in-time(JIT, 即时编译)，即 python 代码运行的时候再去编译cpp和cuda文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心部分</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line">cuda_module = load(name=<span class="string">&quot;add2&quot;</span>,</span><br><span class="line">                           extra_include_paths=[<span class="string">&quot;include&quot;</span>],</span><br><span class="line">                           sources=[<span class="string">&quot;kernel/add2_ops.cpp&quot;</span>, <span class="string">&quot;kernel/add2_kernel.cu&quot;</span>],</span><br><span class="line">                           verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">cuda_module.torch_launch_add2(cuda_c, a, b, n)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run_time.py</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> argparse       <span class="comment"># 导入argparse模块，用于命令行参数解析</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line">cuda_module = load(name=<span class="string">&quot;add2&quot;</span>,</span><br><span class="line">                    extra_include_paths=[<span class="string">&quot;include&quot;</span>],</span><br><span class="line">                    sources=[<span class="string">&quot;kernel/add2_ops.cpp&quot;</span>, <span class="string">&quot;kernel/add2_kernel.cu&quot;</span>],</span><br><span class="line">                    verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># c = a + b (shape: [n * n])</span></span><br><span class="line">n = <span class="number">1024</span></span><br><span class="line">a = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">b = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">cuda_c = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line"></span><br><span class="line">ntest = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># func是一个函数，这个类似c++传函数指针</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_time</span>(<span class="params">func</span>):</span><br><span class="line">    times = <span class="built_in">list</span>()</span><br><span class="line">    res = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 对GPU进行预热，避免首次运行时性能不稳定</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        res = func()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(ntest):</span><br><span class="line">        <span class="comment"># 同步线程以获得准确的cuda运行时间</span></span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        <span class="comment"># 第一个time是import的time，即模块名，第二个是函数名，返回当前时间的时间戳</span></span><br><span class="line">        start_time = time.time()   </span><br><span class="line">        func()</span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        end_time = time.time()</span><br><span class="line">        times.append((end_time-start_time)*<span class="number">1e6</span>)</span><br><span class="line">    <span class="keyword">return</span> times, res</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_cuda</span>():</span><br><span class="line">    cuda_module.torch_launch_add2(cuda_c, a, b, n)</span><br><span class="line">    <span class="keyword">return</span> cuda_c</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_torch</span>():</span><br><span class="line">    c = a + b</span><br><span class="line">    <span class="keyword">return</span> c.contiguous() <span class="comment"># 确保返回的结果在内存中是连续的</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running cuda...&quot;</span>)</span><br><span class="line">    cuda_time, cuda_res = show_time(run_cuda)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Cuda time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(cuda_time)))</span><br><span class="line">    <span class="comment"># np.mean() 返回算数平均值</span></span><br><span class="line">    <span class="comment"># &quot;&#123;:.3f&#125;&quot;format 类似c++</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running torch...&quot;</span>)</span><br><span class="line">    torch_time, torch_res = show_time(run_torch)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Torch time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(torch_time)))</span><br><span class="line"></span><br><span class="line">    torch.allclose(cuda_res, torch_res)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Kernel test passed.&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行如下命令即可</span></span><br><span class="line">python run_time.py</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">运行结果</span></span><br><span class="line">Using /home/xxx/.cache/torch_extensions as PyTorch extensions root...</span><br><span class="line">Detected CUDA files, patching ldflags</span><br><span class="line">Emitting ninja build file /home/xxx/.cache/torch_extensions/add2/build.ninja...</span><br><span class="line">Building extension module add2...</span><br><span class="line">Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)</span><br><span class="line">Loading extension module add2...</span><br><span class="line">Running cuda...</span><br><span class="line">Cuda time:  53.453us</span><br><span class="line">Running torch...</span><br><span class="line">Torch time:  59.795us</span><br><span class="line">Kernel test passed.</span><br></pre></td></tr></table></figure>
<h3 id="SETUP-编译调用"><a href="#SETUP-编译调用" class="headerlink" title="SETUP 编译调用"></a>SETUP 编译调用</h3><p>&emsp;&emsp;第二种编译的方式是通过Setuptools，也就是编写<code>setup.py</code>，之后运行这个脚本。就可以将<code>add2</code>作为一个包添加到你的环境下，之后就可以像<code>import numpy</code>一样导入了，非常方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setup.py</span></span><br><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> BuildExtension, CUDAExtension</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&quot;add2&quot;</span>,</span><br><span class="line">    include_dirs=[<span class="string">&quot;include&quot;</span>],</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            <span class="string">&quot;add2&quot;</span>,</span><br><span class="line">            [<span class="string">&quot;kernel/add2_ops.cpp&quot;</span>, <span class="string">&quot;kernel/add2_kernel.cu&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">    ],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&quot;build_ext&quot;</span>: BuildExtension</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行如下命令安装</span></span><br><span class="line">python setup.py install</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行结果</span></span><br><span class="line">Processing dependencies for add2==0.0.0</span><br><span class="line">Finished processing dependencies for add2==0.0.0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python 代码中调用</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> add2</span><br><span class="line">add2.torch_launch_add2(c, a, b, n)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run_time.py</span></span><br><span class="line"><span class="keyword">import</span> time   </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> add2</span><br><span class="line"></span><br><span class="line"><span class="comment"># c = a + b (shape: [n * n])</span></span><br><span class="line">n = <span class="number">1024</span></span><br><span class="line">a = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">b = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">cuda_c = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line"></span><br><span class="line">ntest = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_time</span>(<span class="params">func</span>):</span><br><span class="line">    times = <span class="built_in">list</span>()</span><br><span class="line">    res = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        res = func()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(ntest):</span><br><span class="line">        </span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        start_time = time.time()   </span><br><span class="line">        func()</span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        end_time = time.time()</span><br><span class="line">        times.append((end_time-start_time)*<span class="number">1e6</span>)</span><br><span class="line">    <span class="keyword">return</span> times, res</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_cuda</span>():</span><br><span class="line">    add2.torch_launch_add2(cuda_c, a, b, n)</span><br><span class="line">    <span class="keyword">return</span> cuda_c</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_torch</span>():</span><br><span class="line">    c = a + b</span><br><span class="line">    <span class="keyword">return</span> c.contiguous() </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running cuda...&quot;</span>)</span><br><span class="line">    cuda_time, cuda_res = show_time(run_cuda)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Cuda time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(cuda_time)))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running torch...&quot;</span>)</span><br><span class="line">    torch_time, torch_res = show_time(run_torch)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Torch time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(torch_time)))</span><br><span class="line"></span><br><span class="line">    torch.allclose(cuda_res, torch_res)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Kernel test passed.&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">python run_time.py 运行结果</span></span><br><span class="line">Running cuda...</span><br><span class="line">Cuda time:  49.472us</span><br><span class="line">Running torch...</span><br><span class="line">Torch time:  61.989us</span><br><span class="line">Kernel test passed.</span><br></pre></td></tr></table></figure>
<h3 id="CMAKE-编译调用"><a href="#CMAKE-编译调用" class="headerlink" title="CMAKE 编译调用"></a>CMAKE 编译调用</h3><p>&emsp;&emsp;逆天cmake，前两种方法都好用，就你给我报错是吧。受够了，我本来也不会cmake，之前用cmake就经常报错，懒得学了喵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run_time.py 支持两种方法</span></span><br><span class="line"><span class="comment"># python run_time.py --compiler jit 或 python run_time.py --compiler setup</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># c = a + b (shape: [n * n])</span></span><br><span class="line">n = <span class="number">1024</span></span><br><span class="line">a = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">b = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">cuda_c = torch.rand((n,n), device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line"></span><br><span class="line">ntest = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_time</span>(<span class="params">func</span>):</span><br><span class="line">    times = <span class="built_in">list</span>()</span><br><span class="line">    res = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        res = func()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(ntest):</span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        func()</span><br><span class="line">        torch.cuda.synchronize(device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        end_time = time.time()</span><br><span class="line">        times.append((end_time-start_time)*<span class="number">1e6</span>)</span><br><span class="line">    <span class="keyword">return</span> times, res</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_cuda</span>():</span><br><span class="line">    <span class="keyword">if</span> args.compiler == <span class="string">&#x27;jit&#x27;</span>:</span><br><span class="line">        cuda_module.torch_launch_add2(cuda_c, a, b, n)</span><br><span class="line">    <span class="keyword">elif</span> args.compiler == <span class="string">&#x27;setup&#x27;</span>:</span><br><span class="line">        add2.torch_launch_add2(cuda_c, a, b, n)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;Type of cuda compiler must be one of jit/setup.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cuda_c</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_torch</span>():</span><br><span class="line">    c = a + b</span><br><span class="line">    <span class="keyword">return</span> c.contiguous()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># parser 是 ArgumentParser 的一个实例，负责处理命令行输入的参数</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    <span class="comment"># 向命令行解析器添加可选参数</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--compiler&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=[<span class="string">&#x27;jit&#x27;</span>, <span class="string">&#x27;setup&#x27;</span>], default=<span class="string">&#x27;jit&#x27;</span>)</span><br><span class="line">    <span class="comment"># 返回一个包含所有命令行参数值的对象</span></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.compiler == <span class="string">&#x27;jit&#x27;</span>:</span><br><span class="line">        <span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line">        cuda_module = load(name=<span class="string">&quot;add2&quot;</span>,</span><br><span class="line">                           extra_include_paths=[<span class="string">&quot;include&quot;</span>],</span><br><span class="line">                           sources=[<span class="string">&quot;kernel/add2_ops.cpp&quot;</span>, <span class="string">&quot;kernel/add2_kernel.cu&quot;</span>],</span><br><span class="line">                           verbose=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">elif</span> args.compiler == <span class="string">&#x27;setup&#x27;</span>:</span><br><span class="line">        <span class="keyword">import</span> add2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;Type of cuda compiler must be one of jit/setup.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running cuda...&quot;</span>)</span><br><span class="line">    cuda_time, cuda_res = show_time(run_cuda)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Cuda time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(cuda_time)))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running torch...&quot;</span>)</span><br><span class="line">    torch_time, torch_res = show_time(run_torch)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Torch time:  &#123;:.3f&#125;us&quot;</span>.<span class="built_in">format</span>(np.mean(torch_time)))</span><br><span class="line"></span><br><span class="line">    torch.allclose(cuda_res, torch_res)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Kernel test passed.&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python run_time.py --compiler setup</span><br><span class="line">python run_time.py --compiler jit</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/645330027">CUDA（一）：CUDA 编程基础</a></p>
<p>[2]<a target="_blank" rel="noopener" href="https://github.com/ifromeast/cuda_learning/tree/main">https://github.com/ifromeast/cuda_learning/tree/main</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://godweiyang.com/2021/03/18/torch-cpp-cuda/">PyTorch自定义CUDA算子教程与运行时间分析</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://godweiyang.com/2021/03/21/torch-cpp-cuda-2/">详解PyTorch编译并调用自定义CUDA算子的三种方式</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://github.com/godweiyang/NN-CUDA-Example">https://github.com/godweiyang/NN-CUDA-Example</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CUDA%E7%BC%96%E7%A8%8B/" rel="tag"># CUDA编程</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/10/24/%E5%A5%87%E6%80%AA%E7%9A%84%E6%A8%A1%E6%9D%BF/" rel="prev" title="奇怪的东西">
                  <i class="fa fa-angle-left"></i> 奇怪的东西
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/03/cuda-learning-02/" rel="next" title="cuda_learning_02">
                  cuda_learning_02 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Asuka</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
