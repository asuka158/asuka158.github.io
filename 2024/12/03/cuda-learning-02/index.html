<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>


    <meta name="description" content="cuda_learning_02       一、GPU 与 CUDA 结构       二、CUDA编程的要素       三、实践：PyTorch自定义CUDA算子">
<meta property="og:type" content="article">
<meta property="og:title" content="cuda_learning_02">
<meta property="og:url" content="http://example.com/2024/12/03/cuda-learning-02/index.html">
<meta property="og:site_name" content="Asuka">
<meta property="og:description" content="cuda_learning_02       一、GPU 与 CUDA 结构       二、CUDA编程的要素       三、实践：PyTorch自定义CUDA算子">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241203173846140.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241211170806477.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241211173038616.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241211174124976.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241219223345835.png">
<meta property="article:published_time" content="2024-12-03T09:36:37.000Z">
<meta property="article:modified_time" content="2024-12-19T15:04:16.959Z">
<meta property="article:author" content="Asuka">
<meta property="article:tag" content="CUDA编程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241203173846140.png">


<link rel="canonical" href="http://example.com/2024/12/03/cuda-learning-02/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/12/03/cuda-learning-02/","path":"2024/12/03/cuda-learning-02/","title":"cuda_learning_02"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>cuda_learning_02 | Asuka</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Asuka</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">
    cuda_learning_02
</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81GPU%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB"><span class="nav-number">1.1.</span> <span class="nav-text">一、GPU的内存体系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%84%E7%BA%A7%E5%86%85%E5%AD%98%E5%8F%8A%E5%85%B6%E7%89%B9%E7%82%B9"><span class="nav-number">1.1.1.</span> <span class="nav-text">各级内存及其特点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98-global-memory"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">全局内存(global memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98-constant-memory"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">常量内存(constant memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98-texture-memory-%E5%92%8C%E8%A1%A8%E9%9D%A2%E5%86%85%E5%AD%98-surface-memory"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">纹理内存(texture memory)和表面内存(surface memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%84%E5%AD%98%E5%99%A8%EF%BC%88register%EF%BC%89"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">寄存器（register）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E5%86%85%E5%AD%98-local-memory"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">局部内存(local memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-shared-memory"><span class="nav-number">1.1.1.6.</span> <span class="nav-text">共享内存(shared memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L1%E5%92%8CL2-%E7%BC%93%E5%AD%98"><span class="nav-number">1.1.1.7.</span> <span class="nav-text">L1和L2 缓存</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SM-%E6%9E%84%E6%88%90%E5%8F%8A%E5%85%B8%E5%9E%8BGPU%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">1.1.2.</span> <span class="nav-text">SM 构成及典型GPU的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU-%E4%B9%8B%E5%A4%96%EF%BC%9A%E8%BF%91%E5%AD%98%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%AD%98%E7%AE%97%E4%B8%80%E4%BD%93"><span class="nav-number">1.1.3.</span> <span class="nav-text">GPU 之外：近存计算与存算一体</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E9%80%9A%E8%BF%87%E8%A7%84%E7%BA%A6-Reduction-%E6%93%8D%E4%BD%9C%E7%90%86%E8%A7%A3GPU%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB"><span class="nav-number">1.2.</span> <span class="nav-text">二、通过规约(Reduction)操作理解GPU内存体系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#blog1"><span class="nav-number">1.2.1.</span> <span class="nav-text">blog1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-cpu-%E7%89%88%E6%9C%AC"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">1. cpu 版本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BB%85%E4%BD%BF%E7%94%A8%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">2. 仅使用全局内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">3.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%94%E8%AE%B0"><span class="nav-number">1.2.2.</span> <span class="nav-text">笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%80%E5%B7%A71%EF%BC%9A%E8%A7%A3%E5%86%B3warp-divergence"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">技巧1：解决warp divergence</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%80%E5%B7%A72%EF%BC%9A%E8%A7%A3%E5%86%B3bank%E5%86%B2%E7%AA%81"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">技巧2：解决bank冲突</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8A%80%E5%B7%A73%EF%BC%9A%E8%A7%A3%E5%86%B3idle%E7%BA%BF%E7%A8%8B"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">技巧3：解决idle线程</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Asuka"
      src="/images/asuka.jpg">
  <p class="site-author-name" itemprop="name">Asuka</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/03/cuda-learning-02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/asuka.jpg">
      <meta itemprop="name" content="Asuka">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Asuka">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="cuda_learning_02 | Asuka">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cuda_learning_02
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-03 17:36:37" itemprop="dateCreated datePublished" datetime="2024-12-03T17:36:37+08:00">2024-12-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-12-19 23:04:16" itemprop="dateModified" datetime="2024-12-19T23:04:16+08:00">2024-12-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/HPC/" itemprop="url" rel="index"><span itemprop="name">HPC</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p></p><h1 style="text-align: center;">
    cuda_learning_02
</h1><p></p>
<div style="text-align: center;">
    <strong>一、GPU 与 CUDA 结构</strong>
</div>
<div style="text-align: center;">
    <strong>二、CUDA编程的要素</strong>
</div>
<div style="text-align: center;">
    <strong>三、实践：PyTorch自定义CUDA算子</strong>
</div>





<span id="more"></span>
<p>&emsp;&emsp;内容主要来自知乎文章，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654027980">CUDA（二）：GPU的内存体系及其优化指南</a>，本文是学习笔记。</p>
<h2 id="一、GPU的内存体系"><a href="#一、GPU的内存体系" class="headerlink" title="一、GPU的内存体系"></a>一、GPU的内存体系</h2><h3 id="各级内存及其特点"><a href="#各级内存及其特点" class="headerlink" title="各级内存及其特点"></a>各级内存及其特点</h3><p><img src="/2024/12/03/cuda-learning-02/image-20241203173846140.png" alt="image-20241203173846140"></p>
<div align="center" style="color: #aaa;">   CUDA 内存模型的层次结构 </div>

<h4 id="全局内存-global-memory"><a href="#全局内存-global-memory" class="headerlink" title="全局内存(global memory)"></a>全局内存(global memory)</h4><p>&emsp;&emsp;最大，延迟最高、最长使用的内存，常说的“显存”中的大部分都是全局内存。可以用 <code>cudaMemcpy</code>函数将主机的数据复制到全局内存，或者反过来。 </p>
<h4 id="常量内存-constant-memory"><a href="#常量内存-constant-memory" class="headerlink" title="常量内存(constant memory)"></a>常量内存(constant memory)</h4><p>&emsp;&emsp;常量内存是指存储在<strong>片下</strong>存储的<strong>设备内存</strong>，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，常量内存为<strong>只读内存</strong>。 常量内存数量有限，<strong>一共仅有 64 KB</strong>，由于有缓存，常量内存的访问速度比全局内存高，但得到高访问速度的前提是一个线程束中的线程(一个线程块中相邻的 32 个线程)要读取相同的常量内存数据。</p>
<p>&emsp;&emsp;一个使用常量内存的方法是在核函数外面用 <code>__constant__</code> 定义变量，并用 API 函数 <code>cudaMemcpyToSymbol</code> 将数据从主机端复制到设备的常量内存后 供核函数使用。</p>
<h4 id="纹理内存-texture-memory-和表面内存-surface-memory"><a href="#纹理内存-texture-memory-和表面内存-surface-memory" class="headerlink" title="纹理内存(texture memory)和表面内存(surface memory)"></a>纹理内存(texture memory)和表面内存(surface memory)</h4><p>&emsp;&emsp;纹理内存和表面内存<strong>类似于常量内存</strong>，也是一 种<strong>具有缓存</strong>的全局内存，有相同的可见范围和生命周期，而且一般仅可读(表面内存也可 写)。不同的是，纹理内存和表面内存<strong>容量更大</strong>，而且使用方式和常量内存也不一样。</p>
<h4 id="寄存器（register）"><a href="#寄存器（register）" class="headerlink" title="寄存器（register）"></a>寄存器（register）</h4><p>&emsp;&emsp;寄存器是线程能独立访问的资源，它所在的位置与局部内存不一样，是在<strong>片上（on chip）的存储</strong>，用来存储一些线程的暂存数据。寄存器的速度是访问中<strong>最快</strong>的，但是它的容量较小。</p>
<p>&emsp;&emsp;在<strong>核函数中定义的不加任何限定符的变量</strong>一般来说就存放于寄存器(register)中。 各种<strong>内建变量</strong>，如 gridDim、blockDim、blockIdx、 threadIdx 及 warpSize 都保存在特殊的寄存器中，以便高效访问。</p>
<p>&emsp;&emsp;寄存器变量仅仅被一个线程可见，寄存器的生命周期也与所属线程的生命周期 一致，从定义它开始，到线程消失时结束。</p>
<h4 id="局部内存-local-memory"><a href="#局部内存-local-memory" class="headerlink" title="局部内存(local memory)"></a>局部内存(local memory)</h4><p>&emsp;&emsp;局部内存和寄存器几乎一 样，<strong>核函数中定义的不加任何限定符的变量</strong>有<strong>可能</strong>在寄存器中，也有可能在局部内存中。寄存器中<strong>放不下</strong>的变量，以及<strong>索引值不能在编译时就确定的数组</strong>，都有可能放在局部内存中。</p>
<p>&emsp;&emsp;虽然称之为“局部内存”，但是其<strong>本质是设备全局内存</strong>（但不同于全局内存）中为每个线程单独分配的一块内存。所以，局部内存的延迟也很高，每个线程最多能使用高达<strong>512 KB</strong>的局部内存，但使用过多会降低程序的性能。</p>
<h4 id="共享内存-shared-memory"><a href="#共享内存-shared-memory" class="headerlink" title="共享内存(shared memory)"></a>共享内存(shared memory)</h4><p>&emsp;&emsp;共享内存和寄存器类似，存在于<strong>芯片上</strong>，具有<strong>仅次于寄存器的读写速度</strong>，数量也有限。 一个使用共享内存的变量可以 <code>__shared__</code> 修饰符来定义。</p>
<p>&emsp;&emsp;共享内存对整个线程块可见，其生命周期也与整个线程块一致。共享内存的主要作用是减少对全局内存的访问，或者改善对全局内存的访问模式。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241211170806477.png" alt="image-20241211170806477"></p>
<div align="center" style="color: #aaa;">各种内存特征表</div>

<h4 id="L1和L2-缓存"><a href="#L1和L2-缓存" class="headerlink" title="L1和L2 缓存"></a>L1和L2 缓存</h4><p>&emsp;&emsp;每个 SM 都有一个 L1 缓存，所有 SM 共享一个 L2 缓存。L1 和 L2 缓存都被用来存储局部内存和全局内存中的数据，也包括寄存器中溢出的部分，以减少延时。</p>
<p>&emsp;&emsp;从物理结构上来说，在最新的GPU架构中，L1 缓存、纹理缓存及共享内存三者是统一的。但从编程的角度来看，<strong>共享内存是可编程的</strong>缓存(共享内存的使用完全由用户操控)，而<strong>L1 和 L2 缓存是不可编程的</strong>缓存(用户最多能引导编译器做一些选择)。</p>
<h3 id="SM-构成及典型GPU的对比"><a href="#SM-构成及典型GPU的对比" class="headerlink" title="SM 构成及典型GPU的对比"></a>SM 构成及典型GPU的对比</h3><p>一个 GPU 是由多个 SM 构成的。一个 SM 包含如下资源:</p>
<ul>
<li><p>一定数量的寄存器。</p>
</li>
<li><p>一定数量的共享内存。</p>
</li>
<li><p>常量内存的缓存。</p>
</li>
<li><p>纹理和表面内存的缓存。</p>
</li>
<li><p>L1缓存。</p>
</li>
<li><p>线程束调度器(warp scheduler) 。</p>
</li>
<li><p>执行核心，包括:</p>
</li>
<li><ul>
<li>若干整型数运算的核心(INT32) 。</li>
<li>若干单精度浮点数运算的核心(FP32) 。</li>
<li>若干双精度浮点数运算的核心(FP64) 。</li>
<li>若干单精度浮点数超越函数(transcendental functions)的特殊函数单元(Special Function Units，SFUs)。</li>
<li>若干混合精度的张量核心(tensor cores)</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;<strong>单精度浮点数超越函数（transcendental functions）</strong> 的 <strong>特殊函数单元（Special Function Units，SFUs）</strong> 是指一种硬件单元，它专门用于执行一些数学上超越（超出普通代数运算）函数的计算，如三角函数（sin, cos），指数函数（exp），对数函数（log），平方根（sqrt）等。</p>
<p>&emsp;&emsp;<strong>张量核心（Tensor Cores）</strong>：主要用于加速深度学习中的矩阵运算，尤其是低精度浮点数运算（如 FP16）。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241211173038616.png" alt="image-20241211173038616"></p>
<div align="center" style="color: #aaa;">H100 的 SM 结构图</div>

<ol>
<li><p><strong>L1 Instruction Cache</strong></p>
<p>用于缓存线程束（warp）的指令，提高指令读取的效率。</p>
</li>
<li><p><strong>Warp Scheduler 和 Dispatch Unit</strong></p>
<ul>
<li><p><strong>Warp Scheduler（线程束调度器）</strong>:</p>
<p>每个 SM 中有多个 Warp Scheduler，每个调度器可以每个时钟周期调度 32 个线程（一个 Warp）。调度线程束执行任务，包括加载指令、分配执行单元等。</p>
</li>
<li><p><strong>Dispatch Unit（指令派发单元）</strong>：</p>
<p>Warp Scheduler 将指令分配给不同的执行单元（如 FP32、FP64、INT32 核心或 SFUs 等），由 Dispatch Unit 具体派发。</p>
</li>
</ul>
</li>
<li><p><strong>Register File（寄存器文件）</strong></p>
</li>
<li><p><strong>执行核心</strong></p>
</li>
<li><p><strong>L0 Instruction Cache 和数据缓存</strong></p>
<ul>
<li><p><strong>L0 Instruction Cache</strong>：</p>
<p>每个 Warp Scheduler 附带的更小的指令缓存，用于加速最近使用的指令。</p>
</li>
<li><p><strong>L1 Data Cache / Shared Memory</strong>：</p>
<p>每个 SM 配备 256 KB 的共享内存或 L1 数据缓存。</p>
</li>
</ul>
</li>
<li><p><strong>Load/Store 单元</strong></p>
<p>负责从全局内存中加载数据或将计算结果存储到全局内存中。</p>
</li>
<li><p><strong>纹理单元（Tex）</strong></p>
<p>纹理单元专门用于处理纹理数据加载，通常在图形渲染中使用。</p>
</li>
<li><p><strong>Tensor Memory Accelerator</strong></p>
<p>针对张量核心操作的特殊加速器，用于处理张量内存的加载和存储。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241211174124976.png" alt="image-20241211174124976"></p>
</li>
</ol>
<div align="center" style="color: #aaa;">主流GPU对比图</div>

<h3 id="GPU-之外：近存计算与存算一体"><a href="#GPU-之外：近存计算与存算一体" class="headerlink" title="GPU 之外：近存计算与存算一体"></a>GPU 之外：近存计算与存算一体</h3><p>&emsp;&emsp;在GPU的层次结构之外，为了降低访存成本，获得更高的性能，近存计算与存算一体逐渐成为热门的方向。</p>
<p>&emsp;&emsp;<strong>近存计算:</strong> Graphcore IPU </p>
<p>&emsp;&emsp;<strong>存算一体:</strong> 后摩智能 H30</p>
<p>&emsp;&emsp;存算一体或者存内计算的核心思想是，通过对存储器单元本身进行算法嵌入，使得计算可以在存储器单元内完成。</p>
<h2 id="二、通过规约-Reduction-操作理解GPU内存体系"><a href="#二、通过规约-Reduction-操作理解GPU内存体系" class="headerlink" title="二、通过规约(Reduction)操作理解GPU内存体系"></a>二、通过规约(Reduction)操作理解GPU内存体系</h2><p>&emsp;&emsp;关于reduce优化，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654027980">CUDA（二）：GPU的内存体系及其优化指南</a>(blog1)和<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426978026">深入浅出GPU优化系列：reduce优化</a>(blog2)都做出了很详细的讲解，两篇文章都写得非常详细，优化的角度也是大同小异的，下面的内容主要是整理两篇文章中提到优化方向，并对所有提到优化角度做一个小小的总结。</p>
<p>&emsp;&emsp;首先，算法reduce即求解$x=x_0 \bigotimes x1 \bigotimes x_2 \bigotimes x_3 \bigotimes … \bigotimes x_n$。其中$\bigotimes$可表示为求sum，min，max，avg等操作，最后获得的输出相比于输入一般维度上会递减。在GPU中，reduce采用了一种树形的计算方式，并且由于GPU没有针对global数据的同步操作，只能针对block的数据进行同步。所以，reduce一般分为两个阶段。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241219223345835.png" alt="image-20241219223345835"></p>
<h3 id="blog1"><a href="#blog1" class="headerlink" title="blog1"></a>blog1</h3><h4 id="1-cpu-版本"><a href="#1-cpu-版本" class="headerlink" title="1. cpu 版本"></a>1. cpu 版本</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">real <span class="title">reduce</span><span class="params">(<span class="type">const</span> real *x, <span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    real sum = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; N; ++n)</span><br><span class="line">    &#123;</span><br><span class="line">        sum += x[n];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span> = 33554432.000000.</span><br><span class="line">mx_time = 935.548340.</span><br><span class="line">mi_time = 530.885864.</span><br><span class="line">tot_time = 11747.090820.</span><br><span class="line">avg_time = 587.354553.</span><br></pre></td></tr></table></figure>
<h4 id="2-仅使用全局内存"><a href="#2-仅使用全局内存" class="headerlink" title="2. 仅使用全局内存"></a>2. 仅使用全局内存</h4><p>&emsp;&emsp;每个线程负责其唯一id对应的那个位置的值的计算，N=1e8，每个线程块有128个线程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> __global__ <span class="title">reduce_global</span><span class="params">(real *d_x, real *d_y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    real *x = d_x + blockDim.x * blockIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = blockDim.x &gt;&gt; <span class="number">1</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid &lt; offset)</span><br><span class="line">        &#123;</span><br><span class="line">            x[tid] += x[tid + offset];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        d_y[blockIdx.x] = x[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3"><a href="#3" class="headerlink" title="3."></a>3.</h4><h3 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h3><h4 id="技巧1：解决warp-divergence"><a href="#技巧1：解决warp-divergence" class="headerlink" title="技巧1：解决warp divergence"></a>技巧1：解决warp divergence</h4><p>&emsp;&emsp;确实能够解决warp divergence，只有当工作线程小于32的时候才会出现warp divergence。需要注意的是这份代码里每个线程并不是负责计算自己的唯一id对应的那个位置的数字每次迭代后的值。</p>
<h4 id="技巧2：解决bank冲突"><a href="#技巧2：解决bank冲突" class="headerlink" title="技巧2：解决bank冲突"></a>技巧2：解决bank冲突</h4><p>&emsp;&emsp;确实会存在bank冲突，也确实是第一次迭代是2路冲突，第二次是4路，再是8路，16路，这是因为bank冲突是针对一个wrap里的线程说的。</p>
<p>&emsp;&emsp;解决方法其实就是从大到小枚举offset。</p>
<h4 id="技巧3：解决idle线程"><a href="#技巧3：解决idle线程" class="headerlink" title="技巧3：解决idle线程"></a>技巧3：解决idle线程</h4>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CUDA%E7%BC%96%E7%A8%8B/" rel="tag"># CUDA编程</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/11/30/cuda-learning-01/" rel="prev" title="cuda_learning_01">
                  <i class="fa fa-angle-left"></i> cuda_learning_01
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/20/VTK-Tutorial/" rel="next" title="VTK Tutorial">
                  VTK Tutorial <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Asuka</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
