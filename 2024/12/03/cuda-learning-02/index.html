<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>


    <meta name="description" content="cuda_learning_02       一、GPU的内存体系       二、通过归约(Reduction)操作理解GPU内存体系">
<meta property="og:type" content="article">
<meta property="og:title" content="cuda_learning_02 内存体系 &amp; reduce优化">
<meta property="og:url" content="http://example.com/2024/12/03/cuda-learning-02/index.html">
<meta property="og:site_name" content="Asuka">
<meta property="og:description" content="cuda_learning_02       一、GPU的内存体系       二、通过归约(Reduction)操作理解GPU内存体系">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241203173846140.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241211170806477.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241211173038616.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241211174124976.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241219223345835.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20250218235311514.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20250220173554158.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20250220235444205.png">
<meta property="og:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20250221003155308.png">
<meta property="article:published_time" content="2024-12-03T09:36:37.000Z">
<meta property="article:modified_time" content="2025-07-23T13:18:40.789Z">
<meta property="article:author" content="Asuka">
<meta property="article:tag" content="CUDA编程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/03/cuda-learning-02/image-20241203173846140.png">


<link rel="canonical" href="http://example.com/2024/12/03/cuda-learning-02/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/12/03/cuda-learning-02/","path":"2024/12/03/cuda-learning-02/","title":"cuda_learning_02 内存体系 & reduce优化"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>cuda_learning_02 内存体系 & reduce优化 | Asuka</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Asuka</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">
    cuda_learning_02
</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81GPU%E7%9A%84%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB"><span class="nav-number">1.1.</span> <span class="nav-text">一、GPU的内存体系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%84%E7%BA%A7%E5%86%85%E5%AD%98%E5%8F%8A%E5%85%B6%E7%89%B9%E7%82%B9"><span class="nav-number">1.1.1.</span> <span class="nav-text">各级内存及其特点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98-global-memory"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">全局内存(global memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98-constant-memory"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">常量内存(constant memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98-texture-memory-%E5%92%8C%E8%A1%A8%E9%9D%A2%E5%86%85%E5%AD%98-surface-memory"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">纹理内存(texture memory)和表面内存(surface memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%84%E5%AD%98%E5%99%A8%EF%BC%88register%EF%BC%89"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">寄存器（register）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E5%86%85%E5%AD%98-local-memory"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">局部内存(local memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-shared-memory"><span class="nav-number">1.1.1.6.</span> <span class="nav-text">共享内存(shared memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L1%E5%92%8CL2-%E7%BC%93%E5%AD%98"><span class="nav-number">1.1.1.7.</span> <span class="nav-text">L1和L2 缓存</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SM-%E6%9E%84%E6%88%90%E5%8F%8A%E5%85%B8%E5%9E%8BGPU%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">1.1.2.</span> <span class="nav-text">SM 构成及典型GPU的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU-%E4%B9%8B%E5%A4%96%EF%BC%9A%E8%BF%91%E5%AD%98%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%AD%98%E7%AE%97%E4%B8%80%E4%BD%93"><span class="nav-number">1.1.3.</span> <span class="nav-text">GPU 之外：近存计算与存算一体</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E9%80%9A%E8%BF%87%E5%BD%92%E7%BA%A6-Reduction-%E6%93%8D%E4%BD%9C%E7%90%86%E8%A7%A3GPU%E5%86%85%E5%AD%98%E4%BD%93%E7%B3%BB"><span class="nav-number">1.2.</span> <span class="nav-text">二、通过归约(Reduction)操作理解GPU内存体系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#blog1"><span class="nav-number">1.2.1.</span> <span class="nav-text">blog1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-cpu-%E7%89%88%E6%9C%AC"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">1. cpu 版本</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BB%85%E4%BD%BF%E7%94%A8%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">2. 仅使用全局内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0%E8%A7%84%E7%BA%A6"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">3. 使用共享内存实现规约</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">4.使用动态共享内存实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E5%85%B6%E4%BB%96%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">5. 其他优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-%E5%8E%9F%E5%AD%90%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.1.5.1.</span> <span class="nav-text">5.1 原子函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E5%90%8C%E6%AD%A5%E3%80%81%E7%BA%BF%E7%A8%8B%E6%9D%9F%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.1.5.2.</span> <span class="nav-text">5.2 线程束同步、线程束函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-3-%E5%8D%8F%E4%BD%9C%E7%BB%84"><span class="nav-number">1.2.1.5.3.</span> <span class="nav-text">5.3 协作组</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-4-%E8%A7%A3%E5%86%B3idle%E7%BA%BF%E7%A8%8B"><span class="nav-number">1.2.1.5.4.</span> <span class="nav-text">5.4 解决idle线程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-5-%E9%9D%99%E6%80%81%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">1.2.1.5.5.</span> <span class="nav-text">5.5 静态全局内存</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-%E6%80%BB%E7%BB%93"><span class="nav-number">1.2.1.6.</span> <span class="nav-text">6. 总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#blog2"><span class="nav-number">1.2.2.</span> <span class="nav-text">blog2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Baseline%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Baseline算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A71%EF%BC%9A%E8%A7%A3%E5%86%B3warp-divergence"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">优化技巧1：解决warp divergence</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A72%EF%BC%9A%E8%A7%A3%E5%86%B3bank%E5%86%B2%E7%AA%81"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">优化技巧2：解决bank冲突</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A73%EF%BC%9A%E8%A7%A3%E5%86%B3idle%E7%BA%BF%E7%A8%8B"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">优化技巧3：解决idle线程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A74%EF%BC%9A%E5%B1%95%E5%BC%80%E6%9C%80%E5%90%8E%E4%B8%80%E7%BB%B4%E5%87%8F%E5%B0%91%E5%90%8C%E6%AD%A5"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">优化技巧4：展开最后一维减少同步</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A75%EF%BC%9A%E5%AE%8C%E5%85%A8%E5%B1%95%E5%BC%80%E5%87%8F%E5%B0%91%E8%AE%A1%E7%AE%97"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">优化技巧5：完全展开减少计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A76%EF%BC%9A%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AEblock%E6%95%B0%E9%87%8F"><span class="nav-number">1.2.2.7.</span> <span class="nav-text">优化技巧6：合理设置block数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A77%EF%BC%9A%E4%BD%BF%E7%94%A8shuffle%E6%8C%87%E4%BB%A4"><span class="nav-number">1.2.2.8.</span> <span class="nav-text">优化技巧7：使用shuffle指令</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.2.2.9.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%90%8E%E7%9A%84%E6%9C%80%E5%90%8E"><span class="nav-number">1.2.3.</span> <span class="nav-text">最后的最后</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Update"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">Update</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.3.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Asuka"
      src="/images/asuka.jpg">
  <p class="site-author-name" itemprop="name">Asuka</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">41</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/03/cuda-learning-02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/asuka.jpg">
      <meta itemprop="name" content="Asuka">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Asuka">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="cuda_learning_02 内存体系 & reduce优化 | Asuka">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cuda_learning_02 内存体系 & reduce优化
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-03 17:36:37" itemprop="dateCreated datePublished" datetime="2024-12-03T17:36:37+08:00">2024-12-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-23 21:18:40" itemprop="dateModified" datetime="2025-07-23T21:18:40+08:00">2025-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/HPC/" itemprop="url" rel="index"><span itemprop="name">HPC</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p></p><h1 style="text-align: center;">
    cuda_learning_02
</h1><p></p>
<div style="text-align: center;">
    <strong>一、GPU的内存体系</strong>
</div>
<div style="text-align: center;">
    <strong>二、通过归约(Reduction)操作理解GPU内存体系</strong>
</div>






<span id="more"></span>
<p>&emsp;&emsp;内容主要来自知乎文章，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654027980">CUDA（二）：GPU的内存体系及其优化指南</a>，本文是学习笔记。</p>
<h2 id="一、GPU的内存体系"><a href="#一、GPU的内存体系" class="headerlink" title="一、GPU的内存体系"></a>一、GPU的内存体系</h2><h3 id="各级内存及其特点"><a href="#各级内存及其特点" class="headerlink" title="各级内存及其特点"></a>各级内存及其特点</h3><p><img src="/2024/12/03/cuda-learning-02/image-20241203173846140.png" alt="image-20241203173846140"></p>
<div align="center" style="color: #aaa;">   CUDA 内存模型的层次结构 </div>

<h4 id="全局内存-global-memory"><a href="#全局内存-global-memory" class="headerlink" title="全局内存(global memory)"></a>全局内存(global memory)</h4><p>&emsp;&emsp;最大，延迟最高、最长使用的内存，常说的“显存”中的大部分都是全局内存。可以用 <code>cudaMemcpy</code>函数将主机的数据复制到全局内存，或者反过来。 </p>
<h4 id="常量内存-constant-memory"><a href="#常量内存-constant-memory" class="headerlink" title="常量内存(constant memory)"></a>常量内存(constant memory)</h4><p>&emsp;&emsp;常量内存是指存储在<strong>片下</strong>存储的<strong>设备内存</strong>，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，常量内存为<strong>只读内存</strong>。 常量内存数量有限，<strong>一共仅有 64 KB</strong>，由于有缓存，常量内存的访问速度比全局内存高，但得到高访问速度的前提是一个线程束中的线程(一个线程块中相邻的 32 个线程)要读取相同的常量内存数据。</p>
<p>&emsp;&emsp;一个使用常量内存的方法是在核函数外面用 <code>__constant__</code> 定义变量，并用 API 函数 <code>cudaMemcpyToSymbol</code> 将数据从主机端复制到设备的常量内存后 供核函数使用。</p>
<h4 id="纹理内存-texture-memory-和表面内存-surface-memory"><a href="#纹理内存-texture-memory-和表面内存-surface-memory" class="headerlink" title="纹理内存(texture memory)和表面内存(surface memory)"></a>纹理内存(texture memory)和表面内存(surface memory)</h4><p>&emsp;&emsp;纹理内存和表面内存<strong>类似于常量内存</strong>，也是一 种<strong>具有缓存</strong>的全局内存，有相同的可见范围和生命周期，而且一般仅可读(表面内存也可 写)。不同的是，纹理内存和表面内存<strong>容量更大</strong>，而且使用方式和常量内存也不一样。</p>
<h4 id="寄存器（register）"><a href="#寄存器（register）" class="headerlink" title="寄存器（register）"></a>寄存器（register）</h4><p>&emsp;&emsp;寄存器是线程能独立访问的资源，它所在的位置与局部内存不一样，是在<strong>片上（on chip）的存储</strong>，用来存储一些线程的暂存数据。寄存器的速度是访问中<strong>最快</strong>的，但是它的容量较小。</p>
<p>&emsp;&emsp;在<strong>核函数中定义的不加任何限定符的变量</strong>一般来说就存放于寄存器(register)中。 各种<strong>内建变量</strong>，如 gridDim、blockDim、blockIdx、 threadIdx 及 warpSize 都保存在特殊的寄存器中，以便高效访问。</p>
<p>&emsp;&emsp;寄存器变量仅仅被一个线程可见，寄存器的生命周期也与所属线程的生命周期 一致，从定义它开始，到线程消失时结束。</p>
<h4 id="局部内存-local-memory"><a href="#局部内存-local-memory" class="headerlink" title="局部内存(local memory)"></a>局部内存(local memory)</h4><p>&emsp;&emsp;局部内存和寄存器几乎一 样，<strong>核函数中定义的不加任何限定符的变量</strong>有<strong>可能</strong>在寄存器中，也有可能在局部内存中。寄存器中<strong>放不下</strong>的变量，以及<strong>索引值不能在编译时就确定的数组</strong>，都有可能放在局部内存中。</p>
<p>&emsp;&emsp;虽然称之为“局部内存”，但是其<strong>本质是设备全局内存</strong>（但不同于全局内存）中为每个线程单独分配的一块内存。所以，局部内存的延迟也很高，每个线程最多能使用高达<strong>512 KB</strong>的局部内存，但使用过多会降低程序的性能。</p>
<h4 id="共享内存-shared-memory"><a href="#共享内存-shared-memory" class="headerlink" title="共享内存(shared memory)"></a>共享内存(shared memory)</h4><p>&emsp;&emsp;共享内存和寄存器类似，存在于<strong>芯片上</strong>，具有<strong>仅次于寄存器的读写速度</strong>，数量也有限。 一个使用共享内存的变量可以 <code>__shared__</code> 修饰符来定义。</p>
<p>&emsp;&emsp;共享内存对整个线程块可见，其生命周期也与整个线程块一致。共享内存的主要作用是减少对全局内存的访问，或者改善对全局内存的访问模式。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241211170806477.png" alt="image-20241211170806477"></p>
<div align="center" style="color: #aaa;">各种内存特征表</div>

<h4 id="L1和L2-缓存"><a href="#L1和L2-缓存" class="headerlink" title="L1和L2 缓存"></a>L1和L2 缓存</h4><p>&emsp;&emsp;每个 SM 都有一个 L1 缓存，所有 SM 共享一个 L2 缓存。L1 和 L2 缓存都被用来存储局部内存和全局内存中的数据，也包括寄存器中溢出的部分，以减少延时。</p>
<p>&emsp;&emsp;从物理结构上来说，在最新的GPU架构中，L1 缓存、纹理缓存及共享内存三者是统一的。但从编程的角度来看，<strong>共享内存是可编程的</strong>缓存(共享内存的使用完全由用户操控)，而<strong>L1 和 L2 缓存是不可编程的</strong>缓存(用户最多能引导编译器做一些选择)。</p>
<h3 id="SM-构成及典型GPU的对比"><a href="#SM-构成及典型GPU的对比" class="headerlink" title="SM 构成及典型GPU的对比"></a>SM 构成及典型GPU的对比</h3><p>一个 GPU 是由多个 SM 构成的。一个 SM 包含如下资源:</p>
<ul>
<li><p>一定数量的寄存器。</p>
</li>
<li><p>一定数量的共享内存。</p>
</li>
<li><p>常量内存的缓存。</p>
</li>
<li><p>纹理和表面内存的缓存。</p>
</li>
<li><p>L1缓存。</p>
</li>
<li><p>线程束调度器(warp scheduler) 。</p>
</li>
<li><p>执行核心，包括:</p>
</li>
<li><ul>
<li>若干整型数运算的核心(INT32) 。</li>
<li>若干单精度浮点数运算的核心(FP32) 。</li>
<li>若干双精度浮点数运算的核心(FP64) 。</li>
<li>若干单精度浮点数超越函数(transcendental functions)的特殊函数单元(Special Function Units，SFUs)。</li>
<li>若干混合精度的张量核心(tensor cores)</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;<strong>单精度浮点数超越函数（transcendental functions）</strong> 的 <strong>特殊函数单元（Special Function Units，SFUs）</strong> 是指一种硬件单元，它专门用于执行一些数学上超越（超出普通代数运算）函数的计算，如三角函数（sin, cos），指数函数（exp），对数函数（log），平方根（sqrt）等。</p>
<p>&emsp;&emsp;<strong>张量核心（Tensor Cores）</strong>：主要用于加速深度学习中的矩阵运算，尤其是低精度浮点数运算（如 FP16）。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241211173038616.png" alt="image-20241211173038616"></p>
<div align="center" style="color: #aaa;">H100 的 SM 结构图</div>

<ol>
<li><p><strong>L1 Instruction Cache</strong></p>
<p>用于缓存线程束（warp）的指令，提高指令读取的效率。</p>
</li>
<li><p><strong>Warp Scheduler 和 Dispatch Unit</strong></p>
<ul>
<li><p><strong>Warp Scheduler（线程束调度器）</strong>:</p>
<p>每个 SM 中有多个 Warp Scheduler，每个调度器可以每个时钟周期调度 32 个线程（一个 Warp）。调度线程束执行任务，包括加载指令、分配执行单元等。</p>
</li>
<li><p><strong>Dispatch Unit（指令派发单元）</strong>：</p>
<p>Warp Scheduler 将指令分配给不同的执行单元（如 FP32、FP64、INT32 核心或 SFUs 等），由 Dispatch Unit 具体派发。</p>
</li>
</ul>
</li>
<li><p><strong>Register File（寄存器文件）</strong></p>
</li>
<li><p><strong>执行核心</strong></p>
</li>
<li><p><strong>L0 Instruction Cache 和数据缓存</strong></p>
<ul>
<li><p><strong>L0 Instruction Cache</strong>：</p>
<p>每个 Warp Scheduler 附带的更小的指令缓存，用于加速最近使用的指令。</p>
</li>
<li><p><strong>L1 Data Cache / Shared Memory</strong>：</p>
<p>每个 SM 配备 256 KB 的共享内存或 L1 数据缓存。</p>
</li>
</ul>
</li>
<li><p><strong>Load/Store 单元</strong></p>
<p>负责从全局内存中加载数据或将计算结果存储到全局内存中。</p>
</li>
<li><p><strong>纹理单元（Tex）</strong></p>
<p>纹理单元专门用于处理纹理数据加载，通常在图形渲染中使用。</p>
</li>
<li><p><strong>Tensor Memory Accelerator</strong></p>
<p>针对张量核心操作的特殊加速器，用于处理张量内存的加载和存储。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241211174124976.png" alt="image-20241211174124976"></p>
</li>
</ol>
<div align="center" style="color: #aaa;">主流GPU对比图</div>

<h3 id="GPU-之外：近存计算与存算一体"><a href="#GPU-之外：近存计算与存算一体" class="headerlink" title="GPU 之外：近存计算与存算一体"></a>GPU 之外：近存计算与存算一体</h3><p>&emsp;&emsp;在GPU的层次结构之外，为了降低访存成本，获得更高的性能，近存计算与存算一体逐渐成为热门的方向。</p>
<p>&emsp;&emsp;<strong>近存计算:</strong> Graphcore IPU </p>
<p>&emsp;&emsp;<strong>存算一体:</strong> 后摩智能 H30</p>
<p>&emsp;&emsp;存算一体或者存内计算的核心思想是，通过对存储器单元本身进行算法嵌入，使得计算可以在存储器单元内完成。</p>
<h2 id="二、通过归约-Reduction-操作理解GPU内存体系"><a href="#二、通过归约-Reduction-操作理解GPU内存体系" class="headerlink" title="二、通过归约(Reduction)操作理解GPU内存体系"></a>二、通过归约(Reduction)操作理解GPU内存体系</h2><p>&emsp;&emsp;关于reduce优化，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654027980">CUDA（二）：GPU的内存体系及其优化指南</a>(blog1)和<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426978026">深入浅出GPU优化系列：reduce优化</a>(blog2)都做出了很详细的讲解，两篇文章都写得非常详细，优化的角度也是大同小异的，下面的内容主要是整理两篇文章中提到优化方向，并对所有提到优化角度做一个小小的总结。</p>
<p>&emsp;&emsp;首先，算法reduce即求解$x=x_0 \bigotimes x1 \bigotimes x_2 \bigotimes x_3 \bigotimes … \bigotimes x_n$。其中$\bigotimes$可表示为求sum，min，max，avg等操作，最后获得的输出相比于输入一般维度上会递减。在GPU中，reduce采用了一种树形的计算方式，并且由于GPU没有针对global数据的同步操作，只能针对block的数据进行同步。所以，reduce一般分为两个阶段。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20241219223345835.png" alt="image-20241219223345835"></p>
<div align="center" style="color: #aaa;">归约问题</div>

<h3 id="blog1"><a href="#blog1" class="headerlink" title="blog1"></a>blog1</h3><h4 id="1-cpu-版本"><a href="#1-cpu-版本" class="headerlink" title="1. cpu 版本"></a>1. cpu 版本</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">real <span class="title">reduce</span><span class="params">(<span class="type">const</span> real *x, <span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    real sum = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; N; ++n)</span><br><span class="line">    &#123;</span><br><span class="line">        sum += x[n];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span> = 33554432.000000.</span><br><span class="line">mx_time = 935.548340.</span><br><span class="line">mi_time = 530.885864.</span><br><span class="line">tot_time = 11747.090820.</span><br><span class="line">avg_time = 587.354553.</span><br></pre></td></tr></table></figure>
<h4 id="2-仅使用全局内存"><a href="#2-仅使用全局内存" class="headerlink" title="2. 仅使用全局内存"></a>2. 仅使用全局内存</h4><p>&emsp;&emsp;每个线程负责其唯一id对应的那个位置的值的计算，N=1e8，每个线程块有128个线程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> __global__ <span class="title">reduce_global</span><span class="params">(real *d_x, real *d_y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    real *x = d_x + blockDim.x * blockIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = blockDim.x &gt;&gt; <span class="number">1</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid &lt; offset)</span><br><span class="line">        &#123;</span><br><span class="line">            x[tid] += x[tid + offset];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        d_y[blockIdx.x] = x[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-使用共享内存实现规约"><a href="#3-使用共享内存实现规约" class="headerlink" title="3. 使用共享内存实现规约"></a>3. 使用共享内存实现规约</h4><p>&emsp;&emsp;共享内存的带宽远大于全局内存，和上面的代码主要区别如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">__shared__ real s_y[<span class="number">128</span>];</span><br><span class="line">s_y[tid] = (idx &lt; N) ? d_x[idx] : <span class="number">0.0</span>;</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;完整代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> __global__ <span class="title">reduce_shared</span><span class="params">(real *d_x, real *d_y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    __shared__ real s_y[<span class="number">128</span>];</span><br><span class="line">    s_y[tid] = (idx &lt; N) ? d_x[idx] : <span class="number">0.0</span>;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = blockDim.x &gt;&gt; <span class="number">1</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (tid &lt; offset)</span><br><span class="line">        &#123;</span><br><span class="line">            s_y[tid] += s_y[tid + offset];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        d_y[blockIdx.x] = s_y[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;使用共享内存相对于仅使用全局内存还有两个好处: 一个是<strong>不再要求全局内存数组的长度 N 是线程块大小的整数倍</strong>，另一个是<strong>在规约的过程中不会改变全局内存数组中的数据</strong>(在仅使用全局内存时，数组 d_x 中的部分元素被改变)。</p>
<h4 id="4-使用动态共享内存实现"><a href="#4-使用动态共享内存实现" class="headerlink" title="4.使用动态共享内存实现"></a>4.使用动态共享内存实现</h4><p>&emsp;&emsp;上边使用共享内存数组时，指定了一个固定的长度(128,即blockDim.x)。 这种静态的方式可能会导致错误的发生，因此有必要使用动态操作。</p>
<p>&emsp;&emsp;只需要修改两个地方：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.调用核函数的执行配置中写下第三个参数</span></span><br><span class="line">&lt;&lt;&lt;grid_size, block_size, <span class="built_in">sizeof</span>(real) * block_size&gt;&gt;&gt;</span><br><span class="line"><span class="comment">//2.加上限定词extern，不指定数组大小</span></span><br><span class="line"><span class="keyword">extern</span> __shared__ real s_y[];  </span><br></pre></td></tr></table></figure>
<h4 id="5-其他优化方法"><a href="#5-其他优化方法" class="headerlink" title="5. 其他优化方法"></a>5. 其他优化方法</h4><h5 id="5-1-原子函数"><a href="#5-1-原子函数" class="headerlink" title="5.1 原子函数"></a>5.1 原子函数</h5><p>&emsp;&emsp;前面几个版本的归约函数中，内核函数只是将一个较长的数组<code>d_x</code>变成了一个较短的数组<code>d_y</code>,而对后面这个较短数组的归约过程实际上是在cpu上进行的。而在cpu上进行计算花费的时间占总共计算时间的大部分。</p>
<p>&emsp;&emsp;所以说如果能在GPU计算出最终的结果，则有望显著地减少整体的计算时间。有两种方法能够在 GPU 中得到最终结果，一是用另一个核函数将较短的数组进一步归约，得到最终结果; 二是在先前的核函数的末尾利用<strong>原子函数</strong>进行归约，直接得到最终结果。</p>
<p>&emsp;&emsp;之前的写法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line">    d_y[blockIdx.x] = s_y[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;改成：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (tid == <span class="number">0</span>) </span><br><span class="line">&#123;</span><br><span class="line">    d_y[<span class="number">0</span>] += s_y[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;但是这个过程是可能出现读写冲突的，所以需要使用原子操作。故实际写法如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//atomicAdd(address, val) 待累加变量的地址 address，累加的值 val。</span></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">atomicAdd</span>(d_y, s_y[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="5-2-线程束同步、线程束函数"><a href="#5-2-线程束同步、线程束函数" class="headerlink" title="5.2 线程束同步、线程束函数"></a>5.2 线程束同步、线程束函数</h5><p>&emsp;&emsp;线程束(warp) 是 SM 中基本的执行单元。一个线程束由32个连续线程组成，这些线程按照单指令多线程(SIMT)方式执行。这样如果在条件语句中，同一线程束中的线程执行不同的指令，就会发生<strong>线程束分化(warp divergence)</strong> ，导致性能出现明显下降。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20250218235311514.png" alt="image-20250218235311514"></p>
<div align="center" style="color: #aaa;">线程束分化</div>

<p>&emsp;&emsp;在归约问题中，当所涉及的线程都在一个线程束内时，可以将线程块同步函 数 <code>__syncthreads</code> 换成一个更加廉价的线程束同步函数 <code>__syncwarp</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = blockDim.x &gt;&gt; <span class="number">1</span>; offset &gt;= <span class="number">32</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; offset)</span><br><span class="line">    &#123;</span><br><span class="line">        s_y[tid] += s_y[tid + offset];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; offset)</span><br><span class="line">    &#123;</span><br><span class="line">        s_y[tid] += s_y[tid + offset];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncwarp();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;另外还可以利用<strong>线程束洗牌函数</strong>进行归约计算，函数 <code>__shfl_down_sync</code> 的作用是将高线程号的数据平移到低线程号中。<code>__shfl_down_sync</code> 是 warp 级别的操作，硬件支持非常高效，不依赖共享内存，依赖于寄存器操作。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    y += __shfl_down_sync(FULL_MASK, y, offset);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;相比之前的版本，有两处不同。第一，使用速度更快的寄存器而不是共享内存。第二，去掉了束同步函数，这是因为洗牌函数能够自动处理同步与读-写竞争问题。</p>
<h5 id="5-3-协作组"><a href="#5-3-协作组" class="headerlink" title="5.3 协作组"></a>5.3 协作组</h5><p>&emsp;&emsp;<strong>协作组(cooperative groups)</strong>可以看作是线程块和线程束同步机制的推广，它提供了更为灵活的线程协作方式，包括线程块内部的同步与协作、线程块之间的(网格级的)同步与协作及设备之间的同步与协作。</p>
<p>&emsp;&emsp;使用协作组的功能时需要在相关源文件包含如下头文件 ，并导入命名空间：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cooperative_groups;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;可以用函数 tiled_partition 将一个线程块划分为若干片(tile)，每一片构成一个 新的线程组。目前仅仅可以将片的大小设置为 2 的正整数次方且不大于 32。例如，如下语句通过函 数 tiled_partition 将一个线程块分割为我们熟知的线程束:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thread_group g32 = <span class="built_in">tiled_partition</span>(<span class="built_in">this_thread_block</span>(), <span class="number">32</span>); </span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;同时线程块片类型中也有洗牌函数，可以利用线程块片来进行数组归约的计算。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">real y = s_y[tid];</span><br><span class="line"></span><br><span class="line">thread_block_tile&lt;<span class="number">32</span>&gt; g = <span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(<span class="built_in">this_thread_block</span>());</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = g.<span class="built_in">size</span>() &gt;&gt; <span class="number">1</span>; i &gt; <span class="number">0</span>; i &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    y += g.<span class="built_in">shfl_down</span>(y, i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="5-4-解决idle线程"><a href="#5-4-解决idle线程" class="headerlink" title="5.4 解决idle线程"></a>5.4 解决idle线程</h5><p>&emsp;&emsp;在前边的例子中， 我们都使用大小为 128 的线程块，所以当 offset 等于 64 时，只用了 1/2 的线程进行计算，其余线程闲置。当 offset 等于 32 时，只用了 1/4 的线程进行计算，其余线程闲置。最终，当 offset 等于 1 时，只用了 1/128 的线程进行计算，其余线程闲置。归约过程一共用了 log2 128 = 7 步， 故归约过程中线程的平均利用率只有 (1/2 + 1/4 + …)/7 ≈ 1/7 。</p>
<p>&emsp;&emsp;为了提高效率，可以考虑在归约之前将多个全局内存数组的数据累加到一个共享内存数组的一个元素中。 用一个寄存器变量 y，用来在循环 体中对读取的全局内存数据进行累加， 在规约之前，必须将寄存器中的数据复制到共享内存。(简单来说，就是增加每个线程的计算任务，减少线程总数)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">real y = <span class="number">0.0</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> n = bid * blockDim.x + tid; n &lt; N; n += stride)</span><br><span class="line">&#123;</span><br><span class="line">    y += d_x[n];</span><br><span class="line">&#125;</span><br><span class="line">s_y[tid] = y;</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;执行配置设置为&lt;&lt;<10240,128>&gt;&gt;，完整代码如下：</10240,128></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> __global__ <span class="title">reduce_idle</span><span class="params">(<span class="type">const</span> real *d_x, real *d_y, <span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ real s_y[];</span><br><span class="line"></span><br><span class="line">    real y = <span class="number">0.0</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> n = bid * blockDim.x + tid; n &lt; N; n += stride)</span><br><span class="line">    &#123;</span><br><span class="line">        y += d_x[n];</span><br><span class="line">    &#125;</span><br><span class="line">    s_y[tid] = y;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = blockDim.x &gt;&gt; <span class="number">1</span>; offset &gt;= <span class="number">32</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid &lt; offset)</span><br><span class="line">        &#123;</span><br><span class="line">            s_y[tid] += s_y[tid + offset];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    y = s_y[tid];</span><br><span class="line"></span><br><span class="line">    thread_block_tile&lt;<span class="number">32</span>&gt; g = <span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(<span class="built_in">this_thread_block</span>());</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = g.<span class="built_in">size</span>() &gt;&gt; <span class="number">1</span>; i &gt; <span class="number">0</span>; i &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        y += g.<span class="built_in">shfl_down</span>(y, i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">atomicAdd</span>(d_y, y);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">real <span class="title">reduce</span><span class="params">(<span class="type">const</span> real *d_x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ymem = <span class="built_in">sizeof</span>(real) * GRID_SIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> smem = <span class="built_in">sizeof</span>(real) * BLOCK_SIZE;</span><br><span class="line"></span><br><span class="line">    real h_y[<span class="number">1</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    real *d_y;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;d_y, <span class="built_in">sizeof</span>(real)));</span><br><span class="line"></span><br><span class="line">    reduce_cp&lt;&lt;&lt;GRID_SIZE, BLOCK_SIZE, smem&gt;&gt;&gt;(d_x, d_y, N);</span><br><span class="line">    <span class="comment">//reduce_cp&lt;&lt;&lt;1, 1024, sizeof(real) * 1024&gt;&gt;&gt;(d_y, d_y, GRID_SIZE);</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(h_y, d_y, <span class="built_in">sizeof</span>(real), cudaMemcpyDeviceToHost));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(d_y));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> h_y[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;10240*128=1310720，100000000/1310720=76。即每个线程会先计算至少76个数字的和，再执行类似上述的操作。</p>
<h5 id="5-5-静态全局内存"><a href="#5-5-静态全局内存" class="headerlink" title="5.5 静态全局内存"></a>5.5 静态全局内存</h5><p>&emsp;&emsp;在之前的 reduce 函数中，需要为数组 d_x 分配与释放设备内存。实际上，设备内存的分配与释放是比较耗时的。一种优化方案是使用静态全局内存代替这里的动态全局内存。简单来说其实就是代码1比代码2更快。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1</span></span><br><span class="line"><span class="type">int</span> ar[maxn];</span><br><span class="line"><span class="comment">// 2</span></span><br><span class="line"><span class="type">int</span> *ar;</span><br><span class="line">ar = <span class="built_in">malloc</span>(maxn * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;利用函数 cudaGetSymbolAddress 将该指针与静态全局变量 static_x 联系起来。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__device__ real static_x[N];  <span class="comment">//定义在全局</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">fun</span>()</span><br><span class="line">&#123;</span><br><span class="line">    real *d_x;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaGetSymbolAddress</span>((<span class="type">void</span>**)&amp;d_x, static_x));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个方法其实存在一定的局限性，在实际应用中也许数据从cpu到gpu这个过程都是在python代码实现了，导致这个方法无法应用。</p>
<h4 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h4><p>&emsp;&emsp;总结一下整个优化流程。首先是最基本的使用<strong>共享内存</strong>，关于<strong>静态共享内存</strong>还是<strong>动态共享内存</strong>，我认为这个区别并不是很大，但是通常是使用<strong>动态共享内存</strong>。这是泛用性较高、性能提升较大的一个点。</p>
<p>&emsp;&emsp;在这之后存在的问题是可以将长数组归约成短数组（长度等于调用的线程块数），但是依旧没有获得最终答案，此时，如果在cpu计算短数组的话，那么$time_{cpu} &gt;&gt; time_{gpu}$。为解决这个问题，有两个方法，一是再调一个线程块；二是使用<strong>原子操作</strong>，每个线程块算完直接写到最后的答案中，而这个过程会存在读写冲突，所以需要使用<strong>原子操作</strong>。当然原子操作这个技巧在其他问题中也许无法应用，泛用性可能较低，不过在这个问题中带来的性能提升较大。</p>
<p>&emsp;&emsp;之后，为了解决可能存在的<strong>线程束分化</strong>的问题，使用<strong>线程束同步</strong>、<strong>线程束函数</strong>、<strong>协作组</strong>等方法，充分利用<strong>SIMD</strong>的特性，充分利用<strong>寄存器</strong>的带宽实现优化。解决<strong>线程束分化</strong>应该是一个比较重要的点，泛用性较高，带来的性能提升也较大。不过，如果程序本身不存在线程束分化，那么仅仅使用<strong>线程束函数</strong>、<strong>协作组</strong>的话带来的提升可能有限。再者需要解决的问题也不一定有适配的<strong>线程束函数</strong>。</p>
<p>&emsp;&emsp;再者，就是发现线程工作不均衡，有的线程的计算量很小，即<strong>idle线程</strong>，为了解决这一问题，常见的做法就是增加每个线程的工作量（原本繁忙的线程增加的相对少，而idle的线程增加的多），提高线程利用率。这也是一个性能提升较大，泛用性较高的技巧。</p>
<p>&emsp;&emsp;最后，就是关于<strong>静态全局内存</strong>。作为一名<strong>acmer</strong>，开数组的时候都是</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> maxn = <span class="number">1e5</span> + <span class="number">5</span>;</span><br><span class="line"><span class="type">int</span> ar[maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而不是</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> n;</span><br><span class="line">	<span class="type">int</span> *ar;</span><br><span class="line">	cin &gt;&gt; n;</span><br><span class="line">	ar = molloc <span class="keyword">or</span> <span class="keyword">new</span> ...   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>静态全局内存</strong>的优化思路跟上面的做法类似，个人认为对性能的提升是有的，但是泛用性较低。</p>
<p>&emsp;&emsp;最后，附上原作者的实验结果，作为重要参考。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20250220173554158.png" alt="image-20250220173554158"></p>
<div align="center" style="color: #aaa;">各种方法的结果及性能</div>

<h3 id="blog2"><a href="#blog2" class="headerlink" title="blog2"></a>blog2</h3><p>&emsp;&emsp;这篇博客中多次提到如下参数：</p>
<ul>
<li><p><strong>BlockNum</strong>：即开启的block数量，即上面所说的M，代表需要将数组切分为几份。</p>
</li>
<li><p><strong>Thread_per_block</strong>:每个block中开启的线程数，一般而言，取128，256，512，1024这几个参数会比较多。</p>
</li>
<li><p><strong>Num_per_block</strong>:每个block需要进行reduce操作的长度。</p>
</li>
</ul>
<h4 id="Baseline算法"><a href="#Baseline算法" class="headerlink" title="Baseline算法"></a>Baseline算法</h4><p>&emsp;&emsp;基准算法，没什么特别的，但是提到了一些观点。首先，说是<strong>优化的本质是通过软件榨干硬件资源</strong>，所以必须清楚地了解代码在硬件上的执行过程才能更好地进行优化。</p>
<p>&emsp;&emsp;从硬件角度来分析一下代码。为了执行代码，GPU需要分配两种资源，一个是<strong>存储资源</strong>，一个是<strong>计算资源</strong>。<strong>存储资源</strong>包括在global memory、shared memory等存储空间。需要注意的是，<strong>shared memory存在bank冲突的问题，因而需要格外小心</strong>。 <strong>计算资源</strong>其实是根据thread数量来确定的，一个block中分配256个thread线程，32个线程为一组，绑定在一个SIMD单元。256个线程可以简单地理解为分配了8组SIMD单元。（但实际的硬件资源分配不是这样，因为一个SM的计算资源有限，不可能真的给每一个block都分配这么多的SIMD单元。）</p>
<h4 id="优化技巧1：解决warp-divergence"><a href="#优化技巧1：解决warp-divergence" class="headerlink" title="优化技巧1：解决warp divergence"></a>优化技巧1：解决warp divergence</h4><p>&emsp;&emsp;emmm，这篇里的offset从小到大枚举的，上一篇是从大到小枚举的，线程束分化问题比较严重，改成上一篇的枚举顺序其实就好了。</p>
<h4 id="优化技巧2：解决bank冲突"><a href="#优化技巧2：解决bank冲突" class="headerlink" title="优化技巧2：解决bank冲突"></a>优化技巧2：解决bank冲突</h4><p><img src="/2024/12/03/cuda-learning-02/image-20250220235444205.png" alt="image-20250220235444205"></p>
<div align="center" style="color: #aaa;">bank冲突</div>

<p>&emsp;&emsp;首先，什么是bank冲突？可以理解为seme跟线程束一样也是以32为单位，如上图，一行32个元素，而1列是一个bank，每个bank能够并行处理访问请求，但如果多个线程试图访问相同的bank，就会发生冲突，影响性能。</p>
<p>&emsp;&emsp;这篇里的offset从小到大枚举的，确实是会出现bank冲突的，第一次迭代会出现2路bank冲突，第二次迭代会出现4路bank冲突，以此类推。。。</p>
<p>&emsp;&emsp;解决方法就是，offset从大到小枚举。</p>
<h4 id="优化技巧3：解决idle线程"><a href="#优化技巧3：解决idle线程" class="headerlink" title="优化技巧3：解决idle线程"></a>优化技巧3：解决idle线程</h4><p>&emsp;&emsp;方法是将Num_per_block增加一倍。也就是说原来一个block只需要管256个数就行，现在得管512个数了。与blog1中方法类似。</p>
<h4 id="优化技巧4：展开最后一维减少同步"><a href="#优化技巧4：展开最后一维减少同步" class="headerlink" title="优化技巧4：展开最后一维减少同步"></a>优化技巧4：展开最后一维减少同步</h4><p>&emsp;&emsp;存在的问题是多余的线程同步，到最后几轮迭代时，此时的block中只有warp0在干活（blog1中的线程束同步）。</p>
<p>&emsp;&emsp;而这32个线程又是在一个SIMD单元上，存在天然的同步，所以可以把最后一维展开减少同步（循环展开）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="type">float</span>* cache,<span class="type">int</span> tid)</span></span>&#123;</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+32</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+16</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+8</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+4</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+2</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce4</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">32</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)<span class="built_in">warpReduce</span>(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="优化技巧5：完全展开减少计算"><a href="#优化技巧5：完全展开减少计算" class="headerlink" title="优化技巧5：完全展开减少计算"></a>优化技巧5：完全展开减少计算</h4><p>&emsp;&emsp;把for循环完全展开，变成一堆if语句，个人感觉带来的提升是有限的，并且GPU硬件架构的不断发展，NV在编译器上面也做了较多的工作，并且这个优化纯纯是折磨开发者，好好的for循环不写，去写一堆if。</p>
<h4 id="优化技巧6：合理设置block数量"><a href="#优化技巧6：合理设置block数量" class="headerlink" title="优化技巧6：合理设置block数量"></a>优化技巧6：合理设置block数量</h4><p>&emsp;&emsp;引用一下原文：如果一个线程被分配更多的work时，可能会更好地覆盖延时。这一点比较好理解。如果线程有更多的work时，对于编译器而言，就可能有更多的机会对相关指令进行重排，从而去覆盖访存时的巨大延时。虽然这句话并没有很好地说明在某种程度上而言，block少一些会更好。但是，有一点不可否认,<strong>block需要进行合理地设置</strong>。<strong>理论上，block取SM数量的倍数会比较合理</strong>。</p>
<h4 id="优化技巧7：使用shuffle指令"><a href="#优化技巧7：使用shuffle指令" class="headerlink" title="优化技巧7：使用shuffle指令"></a>优化技巧7：使用shuffle指令</h4><p>&emsp;&emsp;Shuffle指令是一组针对warp的指令。Shuffle指令最重要的特性就是<strong>warp内的寄存器可以相互访问</strong>。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>&emsp;&emsp;优化方法和角度和blog1中的大同小异，比较新的一个角度就是<strong>解决bank冲突</strong>，这是一个对性能提升较大，泛用性较高的方法。其他的类似<strong>展开循环</strong>、<strong>合理设置block数量</strong>，相对没那么重要（个人感觉）。</p>
<p>&emsp;&emsp;贴一下原作者的性能对比图。</p>
<p><img src="/2024/12/03/cuda-learning-02/image-20250221003155308.png" alt="image-20250221003155308"></p>
<div align="center" style="color: #aaa;">性能对比</div>

<h3 id="最后的最后"><a href="#最后的最后" class="headerlink" title="最后的最后"></a>最后的最后</h3><p>&emsp;&emsp;代码仓库：<a target="_blank" rel="noopener" href="https://github.com/asuka158/CUDA_Learning">神秘链接。</a></p>
<p>&emsp;&emsp;性能对比：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>计算方法</th>
<th>计算时间（ms）</th>
<th>单次加速比</th>
<th>累计加速比</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>195.956955</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>全局内存</td>
<td>1.8026</td>
<td>108.70</td>
<td>108.70</td>
</tr>
<tr>
<td>静态共享内存</td>
<td>1.8906</td>
<td>0.95</td>
<td>103.65</td>
</tr>
<tr>
<td>动态共享内存</td>
<td>1.8957</td>
<td>1.00</td>
<td>103.37</td>
</tr>
<tr>
<td>原子函数</td>
<td>1.8735</td>
<td>1.01</td>
<td>104.59</td>
</tr>
<tr>
<td>线程束同步函数</td>
<td>1.5082</td>
<td>1.24</td>
<td>129.93</td>
</tr>
<tr>
<td>洗牌函数</td>
<td>1.6358</td>
<td>0.92</td>
<td>119.79</td>
</tr>
<tr>
<td>协作组</td>
<td>1.6705</td>
<td>0.98</td>
<td>117.30</td>
</tr>
<tr>
<td>增大线程利用率</td>
<td>0.68647</td>
<td>2.43</td>
<td>285.46</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;实验设备是2080ti，有一说一，这个结果我是不满意的，有的单次加速比&lt;1，可能是设备的问题？还是我的代码的问题？等我有时间一定找出问题所在，一定说是。</p>
<h4 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h4><p>&emsp;&emsp;回看reduce这部分，虽然写了很多版本的优化，但感觉很多版本的代码意义不大，主要是学习一个优化思路，最近重写了一份新的reduce代码，主要用了 <code>warp shuffle</code> 和 <code>float4</code>，向量化访存其实就可以提高线程利用率，性能接近之前的最佳版本（略优于）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="type">int</span> blockSize&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce_best</span><span class="params">(real *d_x, real *d_y, <span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> s_y[blockSize / <span class="number">32</span>];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> idx = (blockDim.x * blockIdx.x + threadIdx.x) * <span class="number">4</span>;</span><br><span class="line">    <span class="type">int</span> warpId = threadIdx.x / warpSize;</span><br><span class="line">    <span class="type">int</span> laneId = threadIdx.x &amp; (warpSize - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> val = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">if</span>(idx &lt; N) </span><br><span class="line">    &#123;</span><br><span class="line">        float4 tmp_x = <span class="built_in">FLOAT4</span>(d_x[idx]);</span><br><span class="line">        val += tmp_x.x;</span><br><span class="line">        val += tmp_x.y;</span><br><span class="line">        val += tmp_x.z;</span><br><span class="line">        val += tmp_x.w;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> offset = warpSize &gt;&gt; <span class="number">1</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) </span><br><span class="line">        val += __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, val, offset);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(laneId == <span class="number">0</span>) s_y[warpId] = val;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(warpId == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> warpNum = blockDim.x / warpSize;</span><br><span class="line">        val = (laneId &lt; warpNum) ? s_y[laneId] : <span class="number">0.0f</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> offset = warpSize &gt;&gt; <span class="number">1</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) </span><br><span class="line">            val += __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, val, offset);</span><br><span class="line">        <span class="keyword">if</span>(laneId == <span class="number">0</span>) <span class="built_in">atomicAdd</span>(d_y, val);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654027980">CUDA（二）：GPU的内存体系及其优化指南</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426978026">深入浅出GPU优化系列：reduce优化</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//developer.download.nvidia.com/assets/cuda/files/reduction.pdf">Optimizing Parallel Reduction in CUDA</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CUDA%E7%BC%96%E7%A8%8B/" rel="tag"># CUDA编程</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/11/30/cuda-learning-01/" rel="prev" title="cuda_learning_01 CUDA 编程基础">
                  <i class="fa fa-angle-left"></i> cuda_learning_01 CUDA 编程基础
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/20/VTK-Tutorial/" rel="next" title="VTK Tutorial">
                  VTK Tutorial <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Asuka</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
