<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>


    <meta name="description" content="cuda_learning_03       1、前置知识        2、Sgemm       3、Hgemm">
<meta property="og:type" content="article">
<meta property="og:title" content="cuda_learning_03">
<meta property="og:url" content="http://example.com/2025/03/25/cuda-learning-03/index.html">
<meta property="og:site_name" content="Asuka">
<meta property="og:description" content="cuda_learning_03       1、前置知识        2、Sgemm       3、Hgemm">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250630221616375.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250630222331096.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250630223128730.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250630225728596.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250508191629076.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250508194642841.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250508194700041.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250701011406779.png">
<meta property="og:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250701193517088.png">
<meta property="article:published_time" content="2025-03-25T13:39:57.000Z">
<meta property="article:modified_time" content="2025-07-11T21:06:20.935Z">
<meta property="article:author" content="Asuka">
<meta property="article:tag" content="CUDA编程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/03/25/cuda-learning-03/image-20250630221616375.png">


<link rel="canonical" href="http://example.com/2025/03/25/cuda-learning-03/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2025/03/25/cuda-learning-03/","path":"2025/03/25/cuda-learning-03/","title":"cuda_learning_03"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>cuda_learning_03 | Asuka</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Asuka</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">
    cuda_learning_03
</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-number">1.1.</span> <span class="nav-text">1. 前置知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E7%A1%AC%E4%BB%B6%E5%9F%BA%E7%A1%80"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.硬件基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-%E7%AE%97%E8%AE%A1"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">1.1.1 算计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-%E5%B8%A6%E5%AE%BD"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">1.1.2 带宽</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Warp-divergence"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 Warp divergence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%BC%82%E6%AD%A5%E5%A4%8D%E5%88%B6"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 异步复制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E5%85%B7%E4%BD%93%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 具体问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Sgemm"><span class="nav-number">1.2.</span> <span class="nav-text">2. Sgemm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-CPU%E5%AE%9E%E7%8E%B0%E5%92%8CnaiveSgemm"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 CPU实现和naiveSgemm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E7%9F%A9%E9%98%B5%E5%88%86%E5%9D%97%E5%B9%B6%E5%88%A9%E7%94%A8Shared-Memory%E5%92%8CRegisters%EF%BC%88v1%EF%BC%89"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 矩阵分块并利用Shared Memory和Registers（v1）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E8%A7%A3%E5%86%B3-Bank-Conflict-%E9%97%AE%E9%A2%98%EF%BC%88v2%EF%BC%89"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 解决 Bank Conflict 问题（v2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-LDS-32"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">2.3.1 LDS.32</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%AF%B9SMEM%E5%81%9Abank%E5%88%92%E5%88%86"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">2.3.2 为什么要对SMEM做bank划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-3-LDS-64%E5%92%8CLDS-128"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">2.3.3 LDS.64和LDS.128</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E6%B5%81%E6%B0%B4%E5%B9%B6%E8%A1%8C%E5%8C%96%EF%BC%9ADouble-Buffering%EF%BC%88v3%EF%BC%89"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 流水并行化：Double Buffering（v3）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-cuBLAS-%E6%80%A7%E8%83%BD"><span class="nav-number">1.2.5.</span> <span class="nav-text">2.5 cuBLAS 性能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Hgemm"><span class="nav-number">1.3.</span> <span class="nav-text">3. Hgemm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 前置知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-cublas%E7%9A%84%E8%B0%83%E7%94%A8"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">3.1.1 cublas的调用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-tensor-core-api"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">3.1.2 tensor core api</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-ptx-%E6%8C%87%E4%BB%A4"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">3.1.3 ptx 指令</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-v1"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 v1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Hgemm-v2-Global-Memory%E5%88%B0Shared-Memory%E7%9A%84%E5%BC%82%E6%AD%A5%E6%8B%B7%E8%B4%9D"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3 Hgemm v2: Global Memory到Shared Memory的异步拷贝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-v2-Double-Buffer"><span class="nav-number">1.3.4.</span> <span class="nav-text">3.4 v2: Double Buffer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-v4-%E6%8F%90%E9%AB%98L2-Cache%E7%9A%84%E5%B1%80%E9%83%A8%E6%80%A7"><span class="nav-number">1.3.5.</span> <span class="nav-text">3.5 v4: 提高L2 Cache的局部性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-v5-%E5%BE%AA%E7%8E%AF%E5%B1%95%E5%BC%80"><span class="nav-number">1.3.6.</span> <span class="nav-text">3.6 v5: 循环展开</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">1.4.</span> <span class="nav-text">Reference</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Asuka"
      src="/images/asuka.jpg">
  <p class="site-author-name" itemprop="name">Asuka</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">41</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/25/cuda-learning-03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/asuka.jpg">
      <meta itemprop="name" content="Asuka">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Asuka">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="cuda_learning_03 | Asuka">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cuda_learning_03
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-03-25 21:39:57" itemprop="dateCreated datePublished" datetime="2025-03-25T21:39:57+08:00">2025-03-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-12 05:06:20" itemprop="dateModified" datetime="2025-07-12T05:06:20+08:00">2025-07-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/HPC/" itemprop="url" rel="index"><span itemprop="name">HPC</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p></p><h1 style="text-align: center;">
    cuda_learning_03
</h1><p></p>
<div style="text-align: center;">
    <strong>1、前置知识 </strong>
</div>
<div style="text-align: center;">
    <strong>2、Sgemm</strong>
</div>
<div style="text-align: center;">
    <strong>3、Hgemm</strong>
</div>






<span id="more"></span>
<h2 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h2><h3 id="1-1-硬件基础"><a href="#1-1-硬件基础" class="headerlink" title="1.1.硬件基础"></a>1.1.硬件基础</h3><h4 id="1-1-1-算计"><a href="#1-1-1-算计" class="headerlink" title="1.1.1 算计"></a>1.1.1 算计</h4><p> &emsp;查询网站：<a target="_blank" rel="noopener" href="https://www.techpowerup.com/gpu-specs/">https://www.techpowerup.com/gpu-specs/</a></p>
<p><img src="/2025/03/25/cuda-learning-03/image-20250630221616375.png" alt="image-20250630221616375"></p>
<h4 id="1-1-2-带宽"><a href="#1-1-2-带宽" class="headerlink" title="1.1.2 带宽"></a>1.1.2 带宽</h4><p><img src="/2025/03/25/cuda-learning-03/image-20250630222331096.png" alt="image-20250630222331096"></p>
<h3 id="1-2-Warp-divergence"><a href="#1-2-Warp-divergence" class="headerlink" title="1.2 Warp divergence"></a>1.2 Warp divergence</h3><p>&emsp;&emsp;三元表达式不会造成线程束分化说是。</p>
<h3 id="1-3-异步复制"><a href="#1-3-异步复制" class="headerlink" title="1.3 异步复制"></a>1.3 异步复制</h3><p>&emsp;&emsp;可以认为是一种访存方式，指的是<code>__pipeline_memcpy_async()</code>。好处是不使用中间寄存器有助于减少寄存器压力，并可能增加内核占用率。</p>
<p><img src="/2025/03/25/cuda-learning-03/image-20250630223128730.png" alt="image-20250630223128730"></p>
<p>&emsp;&emsp;但是感觉完全可以被<code>cp.async</code>这个ptx指令代替，或者说两者本质是一个东西。后续只会用到<code>cp.async</code>。</p>
<h3 id="1-4-具体问题"><a href="#1-4-具体问题" class="headerlink" title="1.4 具体问题"></a>1.4 具体问题</h3><p>&emsp;&emsp;$C = αAB + βC$</p>
<p>&emsp;&emsp;其中，A的形状是<code>[M, K]</code>,B的形状是<code>[K, N]</code>,C的形状是<code>[M,N]</code>。为了方便，通常取<code>α = 1，β = 0</code>。</p>
<h2 id="2-Sgemm"><a href="#2-Sgemm" class="headerlink" title="2. Sgemm"></a>2. Sgemm</h2><p>&emsp;&emsp;Sgemm的优化，包括4个版本的代码和简要分析，具体分析涉及很多详细的计算分析见Reference。</p>
<h3 id="2-1-CPU实现和naiveSgemm"><a href="#2-1-CPU实现和naiveSgemm" class="headerlink" title="2.1 CPU实现和naiveSgemm"></a>2.1 CPU实现和naiveSgemm</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cpu 实现</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">cpuSgemm</span><span class="params">(<span class="type">float</span> *a, <span class="type">float</span> *b, <span class="type">float</span> *c, <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; M; ++m) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; N; ++n) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">float</span> psum = <span class="number">0.0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; K; ++k) psum += a[<span class="built_in">OFFSET</span>(m, k, K)] * b[<span class="built_in">OFFSET</span>(k, n, N)];</span><br><span class="line">            c[<span class="built_in">OFFSET</span>(m, n, N)] = psum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 朴素的 GPU 实现</span></span><br><span class="line"><span class="comment">//__restrict__ 的作用:开发者向编译器保证：在该指针的作用域内，所有通过该指针访问的内存不会通过其他指针或引用被修改。编译器可以据此假设无别名冲突，从而生成更高效的代码。</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">naiveSgemm</span><span class="params">(<span class="type">float</span>* __restrict__ a, <span class="type">float</span>* __restrict__ b, <span class="type">float</span>* __restrict__ c, <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> K, <span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> tidx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> tidy = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(tidx &lt; N &amp;&amp; tidy &lt; M)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">float</span> sum = <span class="number">0.0</span>;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll </span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">0</span>; k &lt; K; ++k) sum += a[<span class="built_in">OFFSET</span>(tidy, k, K)] * b[<span class="built_in">OFFSET</span>(k, tidx, N)];</span><br><span class="line">        c[<span class="built_in">OFFSET</span>(tidx, tidy, N)] = sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;NVIDIA GeForce RTX 3090的执行结果如下，理论算力$FP32 (TFLOPS)=CUDA 核心数 × 加速频率 × 2$，计算得<code>10,496×1.70×2=35.7 TFLOPS</code>。该方法算力利用率仅有4.5%。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">M N K =    <span class="number">128</span>    <span class="number">128</span>   <span class="number">1024</span>, Time =   <span class="number">0.00008499</span>   <span class="number">0.00008908</span>   <span class="number">0.00009715</span> s, AVG Performance =   <span class="number">376.6978</span> Gflops</span><br><span class="line">M N K =    <span class="number">192</span>    <span class="number">192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00008909</span>   <span class="number">0.00009093</span>   <span class="number">0.00009216</span> s, AVG Performance =   <span class="number">830.2995</span> Gflops</span><br><span class="line">M N K =    <span class="number">256</span>    <span class="number">256</span>   <span class="number">1024</span>, Time =   <span class="number">0.00008909</span>   <span class="number">0.00008970</span>   <span class="number">0.00009114</span> s, AVG Performance =  <span class="number">1496.2557</span> Gflops</span><br><span class="line">M N K =    <span class="number">384</span>    <span class="number">384</span>   <span class="number">1024</span>, Time =   <span class="number">0.00016486</span>   <span class="number">0.00017285</span>   <span class="number">0.00017510</span> s, AVG Performance =  <span class="number">1747.1090</span> Gflops</span><br><span class="line">M N K =    <span class="number">512</span>    <span class="number">512</span>   <span class="number">1024</span>, Time =   <span class="number">0.00031949</span>   <span class="number">0.00032122</span>   <span class="number">0.00032973</span> s, AVG Performance =  <span class="number">1671.3371</span> Gflops</span><br><span class="line">M N K =    <span class="number">768</span>    <span class="number">768</span>   <span class="number">1024</span>, Time =   <span class="number">0.00063898</span>   <span class="number">0.00065208</span>   <span class="number">0.00065434</span> s, AVG Performance =  <span class="number">1852.4714</span> Gflops</span><br><span class="line">M N K =   <span class="number">1024</span>   <span class="number">1024</span>   <span class="number">1024</span>, Time =   <span class="number">0.00106189</span>   <span class="number">0.00106394</span>   <span class="number">0.00106598</span> s, AVG Performance =  <span class="number">2018.4210</span> Gflops</span><br><span class="line">M N K =   <span class="number">1536</span>   <span class="number">1536</span>   <span class="number">1024</span>, Time =   <span class="number">0.00281805</span>   <span class="number">0.00282910</span>   <span class="number">0.00284774</span> s, AVG Performance =  <span class="number">1707.9060</span> Gflops</span><br><span class="line">M N K =   <span class="number">2048</span>   <span class="number">2048</span>   <span class="number">1024</span>, Time =   <span class="number">0.00560026</span>   <span class="number">0.00560722</span>   <span class="number">0.00561152</span> s, AVG Performance =  <span class="number">1531.9420</span> Gflops</span><br><span class="line">M N K =   <span class="number">3072</span>   <span class="number">3072</span>   <span class="number">1024</span>, Time =   <span class="number">0.01216819</span>   <span class="number">0.01321758</span>   <span class="number">0.01334579</span> s, AVG Performance =  <span class="number">1462.2455</span> Gflops</span><br><span class="line">M N K =   <span class="number">4096</span>   <span class="number">4096</span>   <span class="number">1024</span>, Time =   <span class="number">0.02087629</span>   <span class="number">0.02089912</span>   <span class="number">0.02097050</span> s, AVG Performance =  <span class="number">1644.0758</span> Gflops</span><br><span class="line">M N K =   <span class="number">6144</span>   <span class="number">6144</span>   <span class="number">1024</span>, Time =   <span class="number">0.04926976</span>   <span class="number">0.04949125</span>   <span class="number">0.04952781</span> s, AVG Performance =  <span class="number">1562.0824</span> Gflops</span><br><span class="line">M N K =   <span class="number">8192</span>   <span class="number">8192</span>   <span class="number">1024</span>, Time =   <span class="number">0.08597197</span>   <span class="number">0.08599675</span>   <span class="number">0.08600986</span> s, AVG Performance =  <span class="number">1598.1878</span> Gflops</span><br><span class="line">M N K =  <span class="number">12288</span>  <span class="number">12288</span>   <span class="number">1024</span>, Time =   <span class="number">0.19243827</span>   <span class="number">0.19409776</span>   <span class="number">0.19477504</span> s, AVG Performance =  <span class="number">1593.2056</span> Gflops</span><br><span class="line">M N K =  <span class="number">16384</span>  <span class="number">16384</span>   <span class="number">1024</span>, Time =   <span class="number">0.34301132</span>   <span class="number">0.34313369</span>   <span class="number">0.34316391</span> s, AVG Performance =  <span class="number">1602.1622</span> Gflops</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个方法的计算访存比非常低，并且存在大量冗余的全局内存访问。</p>
<h3 id="2-2-矩阵分块并利用Shared-Memory和Registers（v1）"><a href="#2-2-矩阵分块并利用Shared-Memory和Registers（v1）" class="headerlink" title="2.2 矩阵分块并利用Shared Memory和Registers（v1）"></a>2.2 矩阵分块并利用Shared Memory和Registers（v1）</h3><p>&emsp;&emsp;上面提到了naiveSgemm的问题，GPU对Shared Memory和Registers是高于Global Memory的，所以可以利用Shared Memory和Registers作为一个类似于cache的效果，并提高计算访存比，这是一个容易想到的角度，具体要如何做，接下来简要说明。</p>
<p><img src="/2025/03/25/cuda-learning-03/image-20250630225728596.png" alt="image-20250630225728596"></p>
<p>&emsp;&emsp;核心目标可以认为是<strong>提高计算访存比</strong>，具体矩阵分块如上图<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/703256080">（图片来源）</a>。下面详细分析一下线程块等具体的大小选择。</p>
<p>&emsp;&emsp;对于每一个分块：</p>
<p>&emsp;&emsp;计算量：$BM \times BN \times K \times 2$</p>
<p>&emsp;&emsp;访存量：$(BM + BN) \times K \times 4 Byte$</p>
<p>&emsp;&emsp;计算访存比：$\frac{BM \times BN}{2(BM + BN)} = \frac{1}{2(\frac{1}{BN} + \frac{1}{BM})}$</p>
<p>&emsp;&emsp;由上式可知<code>BM</code>和<code>BN</code>越大，计算访存比越高，性能就会越好。但是由于 Shared Memory 容量的限制(3090 1个SM大致96KB)，而一个Block需要占用 $BK \times (BM + BN) \times 4 Bytes$大小。</p>
<p>&emsp;&emsp;TM和TN的取值也受到两方面限制，一方面是线程数的限制，一个Block中有$\frac{BM}{TM} \times \frac{BN}{TN}$个线程，这个数字不能超过1024，且不能太高防止影响SM内Block间的并行；另一方面是寄存器数目的限制，一个线程至少需要<code>TM * TN</code>个寄存器用于存放矩阵 C 的部分和，再加上一些其它的寄存器，所有的寄存器数目不能超过256，且不能太高防止影响SM内同时并行的线程数目。</p>
<p>&emsp;&emsp;最终选取 <code>BM = BN = 128，BK = 8，TM = TN = 8</code>，则此时计算访存比为32。3090的理论算力35.7TFLOPS，理论带宽是936.2 GB/s。不过<strong>实测算力在30TFLOPS左右，实测带宽在789GB/s左右</strong>，所以我认为应该以这两个数据为标准。此时 <code>30TFLOPS/32 = 938GB/s</code>，带宽多少还是会限制计算性能（这个BM等数据的配置是参考其他blog中的最佳数据，但是由于大家设备有所差异，后续再调整实验吧）。</p>
<p>&emsp;&emsp;<strong>关于为什么沿着k维度切成更小的bk，而不是A沿m切，B沿n切？</strong></p>
<p>&emsp;&emsp;因为这样切可以保证每个小A分块和小B分块只会加载一次。</p>
<p>&emsp;&emsp;按理说有一个<strong>中间优化</strong>，就是其他不变，但是每个线程还是只计算一个元素，而不是<code>Tm * Tn</code>。一个线程计算<code>Tm * Tn</code>个原因是：可以减少对shared memory的访存量。</p>
<p>&emsp;&emsp;其实这一步的优化除了提高了计算访存比，使用更快的Shared Memory和Registers，还有涉及到提高硬件利用率的角度（原本一个线程计算一个数据，现在计算<code>Tm * Tn</code>个，即中间优化提到的）。</p>
<p>&emsp;&emsp;根据以上分析，kernel函数实现主要包括以下步骤：</p>
<ul>
<li><p>从Global Memory加载对应的矩阵分块到Shared Memory中。</p>
<p>&emsp;&emsp;每个线程刚好负责A矩阵的4个元素和B矩阵的4个元素，刚好可以用<code>FLOAT4</code>来操作，分配好每个线程负责的元素即可。</p>
</li>
<li><p>计算对应的C矩阵</p>
</li>
<li><p>写回Global Memory</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 矩阵分块、Shared Memory、Registers</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sgemm_V1</span><span class="params">(<span class="type">float</span> * __restrict__ a, <span class="type">float</span> * __restrict__ b, <span class="type">float</span> * __restrict__ c, <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> K, <span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TM = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TN = <span class="number">8</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * blockDim.x + tx;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_a[BM][BK];</span><br><span class="line">    __shared__ <span class="type">float</span> s_b[BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> r_c[TM][TN] = &#123;<span class="number">0.0</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载 到 s_a 的 m 行</span></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = tid &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">1</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从 g_a 的 m 行 加载</span></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> cnt = (K + BK - <span class="number">1</span>) / BK;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> bk = <span class="number">0</span>; i &lt; cnt; ++bk)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = bk * BK + load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_a[load_a_smem_m][load_a_smem_k]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = bk * BK + load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; ++k)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> m = <span class="number">0</span>; m &lt; TM; ++m)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> n = <span class="number">0</span>; n &lt; TN; ++n)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="type">int</span> comp_a_smem_m = ty * TM + m;</span><br><span class="line">                    <span class="type">int</span> comp_b_smem_n = tx * TN + n;</span><br><span class="line">                    r_c[m][n] += s_a[comp_a_smem_m][k] * s_b[k][comp_b_smem_n];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM + i;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; TN; j += <span class="number">4</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN + j;</span><br><span class="line">            <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;计算结果如下,性能达到了理论峰值的54.5%,达到了实际可达峰值的63.3%：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">M N K =    <span class="number">128</span>    <span class="number">128</span>   <span class="number">1024</span>, Time =   <span class="number">0.00019149</span>   <span class="number">0.00019769</span>   <span class="number">0.00020950</span> s, AVG Performance =   <span class="number">169.7357</span> Gflops</span><br><span class="line">M N K =    <span class="number">192</span>    <span class="number">192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00019354</span>   <span class="number">0.00019711</span>   <span class="number">0.00019968</span> s, AVG Performance =   <span class="number">383.0212</span> Gflops</span><br><span class="line">M N K =    <span class="number">256</span>    <span class="number">256</span>   <span class="number">1024</span>, Time =   <span class="number">0.00019866</span>   <span class="number">0.00019927</span>   <span class="number">0.00019968</span> s, AVG Performance =   <span class="number">673.5457</span> Gflops</span><br><span class="line">M N K =    <span class="number">384</span>    <span class="number">384</span>   <span class="number">1024</span>, Time =   <span class="number">0.00019763</span>   <span class="number">0.00020429</span>   <span class="number">0.00020685</span> s, AVG Performance =  <span class="number">1478.2557</span> Gflops</span><br><span class="line">M N K =    <span class="number">512</span>    <span class="number">512</span>   <span class="number">1024</span>, Time =   <span class="number">0.00020480</span>   <span class="number">0.00020531</span>   <span class="number">0.00020582</span> s, AVG Performance =  <span class="number">2614.9028</span> Gflops</span><br><span class="line">M N K =    <span class="number">768</span>    <span class="number">768</span>   <span class="number">1024</span>, Time =   <span class="number">0.00020579</span>   <span class="number">0.00020695</span>   <span class="number">0.00020787</span> s, AVG Performance =  <span class="number">5837.0423</span> Gflops</span><br><span class="line">M N K =   <span class="number">1024</span>   <span class="number">1024</span>   <span class="number">1024</span>, Time =   <span class="number">0.00020787</span>   <span class="number">0.00020920</span>   <span class="number">0.00020992</span> s, AVG Performance = <span class="number">10265.0610</span> Gflops</span><br><span class="line">M N K =   <span class="number">1536</span>   <span class="number">1536</span>   <span class="number">1024</span>, Time =   <span class="number">0.00034406</span>   <span class="number">0.00034447</span>   <span class="number">0.00034509</span> s, AVG Performance = <span class="number">14026.7298</span> Gflops</span><br><span class="line">M N K =   <span class="number">2048</span>   <span class="number">2048</span>   <span class="number">1024</span>, Time =   <span class="number">0.00065843</span>   <span class="number">0.00065996</span>   <span class="number">0.00066150</span> s, AVG Performance = <span class="number">13015.7464</span> Gflops</span><br><span class="line">M N K =   <span class="number">3072</span>   <span class="number">3072</span>   <span class="number">1024</span>, Time =   <span class="number">0.00129741</span>   <span class="number">0.00130109</span>   <span class="number">0.00130253</span> s, AVG Performance = <span class="number">14854.6887</span> Gflops</span><br><span class="line">M N K =   <span class="number">4096</span>   <span class="number">4096</span>   <span class="number">1024</span>, Time =   <span class="number">0.00208794</span>   <span class="number">0.00208978</span>   <span class="number">0.00209408</span> s, AVG Performance = <span class="number">16441.8282</span> Gflops</span><br><span class="line">M N K =   <span class="number">6144</span>   <span class="number">6144</span>   <span class="number">1024</span>, Time =   <span class="number">0.00461210</span>   <span class="number">0.00461885</span>   <span class="number">0.00462234</span> s, AVG Performance = <span class="number">16737.7892</span> Gflops</span><br><span class="line">M N K =   <span class="number">8192</span>   <span class="number">8192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00686694</span>   <span class="number">0.00707564</span>   <span class="number">0.00795443</span> s, AVG Performance = <span class="number">19424.2566</span> Gflops</span><br><span class="line">M N K =  <span class="number">12288</span>  <span class="number">12288</span>   <span class="number">1024</span>, Time =   <span class="number">0.01572765</span>   <span class="number">0.01590118</span>   <span class="number">0.01641571</span> s, AVG Performance = <span class="number">19447.4602</span> Gflops</span><br><span class="line">M N K =  <span class="number">16384</span>  <span class="number">16384</span>   <span class="number">1024</span>, Time =   <span class="number">0.02786099</span>   <span class="number">0.02943130</span>   <span class="number">0.03024384</span> s, AVG Performance = <span class="number">18679.2936</span> Gflops</span><br></pre></td></tr></table></figure>
<h3 id="2-3-解决-Bank-Conflict-问题（v2）"><a href="#2-3-解决-Bank-Conflict-问题（v2）" class="headerlink" title="2.3 解决 Bank Conflict 问题（v2）"></a>2.3 解决 Bank Conflict 问题（v2）</h3><h4 id="2-3-1-LDS-32"><a href="#2-3-1-LDS-32" class="headerlink" title="2.3.1 LDS.32"></a>2.3.1 LDS.32</h4><p>&emsp;&emsp;假设一个warp现在被调度了，它的32个thread此刻要去SMEM上读数。warp发送了一个LDS.32的指令（意思是让所有的thread都取1个数，其大小为4byte，换算成bit就是32）。此时，在cuda的运算中有如下规定：</p>
<ul>
<li><strong>一个warp发送1次取数指令（不一定是LDS.32），它最多只能从SMEM上读取128bytes（32个数）的数据。这是每个warp发送1次取数指令的能取到的数据量上限</strong>。</li>
<li>如果每个thread要取的数，来自不同的bank，我们就认为<strong>没有bank conflict</strong>。warp发送1次指令，所有的threads即可取回自己想要的数据。</li>
<li>来自同一个bank，但是是这个bank上的同一个数（同一个bank的相同地址，或者说是相同layer），此时也<strong>没有bank conflict</strong>（广播机制），也是1次指令。</li>
<li>如果某些threads要取的数，来自同一个bank，并且是这个bank上的不同数（同一个bank的不同地址，即不同layer），此时<strong>发生了bank conflict</strong>。同个warp内的threads想要访问同一个bank下的n个不同的地址，就发生了<strong>n-way bank conflict</strong>（n头bank conflict）。本该1次指令取回的数，就需要串行发送n次指令。</li>
</ul>
<h4 id="2-3-2-为什么要对SMEM做bank划分"><a href="#2-3-2-为什么要对SMEM做bank划分" class="headerlink" title="2.3.2 为什么要对SMEM做bank划分"></a>2.3.2 为什么要对SMEM做bank划分</h4><p>&emsp;&emsp;简单来说，为了均衡banks的路宽，为了<strong>warp间尽量并行</strong>，不要相互阻碍。</p>
<h4 id="2-3-3-LDS-64和LDS-128"><a href="#2-3-3-LDS-64和LDS-128" class="headerlink" title="2.3.3 LDS.64和LDS.128"></a>2.3.3 LDS.64和LDS.128</h4><p>&emsp;&emsp;<strong>LDS.64指令</strong>:一次取8bytes的数，即连续的2个数。</p>
<p>&emsp;&emsp;<strong>LDS.128指令</strong>:一次取16bytes的数，即连续的2个数。</p>
<p>&emsp;&emsp;以LDS.128为例，一个warp需要取128个数，超过了warp单次memory transaction允许的取数上限。所以<strong>该warp会把取数过程拆成4个串行的phase（即4次串行的memory transcation）</strong>：即0～7，8～15，16～23，24～31。<strong>这时bank conflict被定义在每个phase（也就是1/4个warp之内）</strong>。</p>
<p>&emsp;&emsp;接下来，我们分析<code>v1</code>中是否存在<code>bank</code>冲突。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; ++k)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> m = <span class="number">0</span>; m &lt; TM; ++m)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> n = <span class="number">0</span>; n &lt; TN; ++n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> comp_a_smem_m = ty * TM + m;</span><br><span class="line">            <span class="type">int</span> comp_b_smem_n = tx * TN + n;</span><br><span class="line">            r_c[m][n] += s_a[comp_a_smem_m][k] * s_b[k][comp_b_smem_n];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;一个简单且不太严谨的对<code>s_b</code>的访问分析，每个线程需要访问<code>s_b</code>的连续8个元素。那么<code>tid = 0</code>的线程，访问<code>bank_id</code> 为 <code>0，1，2，3，4，5，6，7</code>；<code>tid = 1</code>的线程，访问<code>bank_id</code> 为 <code>8，9，10，11，12，13，14，15</code>；这样的话，就会发现<code>tid = 0</code>和<code>tid = 4</code>的线程访问的bank正好相同，所以存在一个<code>8</code>路bank conflict。</p>
<p>&emsp;&emsp;如果使用<code>FLOAT4</code>，你会发现对<code>s_b</code>的访问存在2路bank conflict，解决方法是把每个线程负责的计算的元素修改一下，让每个线程读取两次连续的4个数，而不是连续的8个数。以上面的图片为例，V1中每个线程块负责计算<code>128*128</code>的子矩阵，每个线程负责计算<code>8*8</code>的子矩阵，线程块大小是<code>16*16</code>。V2中每个线程负责4个<code>4*4</code>的子矩阵，即把<code>128*128</code>的子矩阵分成4个<code>64*64</code>的子矩阵(左上、右上、左下、右下)，每个线程负责的那4个<code>4*4</code>子矩阵在这4个<code>64*64</code>的子矩阵的对应位置。</p>
<p>&emsp;&emsp;对于<code>s_a</code>的访问，v1中不存在bank conflict，因为触发了广播机制。但是v1中的访问方式无法使用<code>FLOAT4</code>，因为对<code>s_a</code>的访问是不连续的，解决办法是转置。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 解决 bank conflict</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sgemm_V2</span><span class="params">(<span class="type">float</span> * __restrict__ a, <span class="type">float</span> * __restrict__ b, <span class="type">float</span> * __restrict__ c, <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * blockDim.x + tx;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_a[BK][BM];</span><br><span class="line">    __shared__ <span class="type">float</span> s_b[BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> r_c[TN][TN] = &#123;<span class="number">0.0</span>&#125;;</span><br><span class="line">    <span class="type">float</span> r_load_a[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_load_b[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_comp_a[TM];</span><br><span class="line">    <span class="type">float</span> r_comp_b[TN];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = tid &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">1</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> cnt = (K + BK - <span class="number">1</span>) / BK;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> bk = <span class="number">0</span>; bk &lt; cnt; ++bk)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = bk * BK + load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = bk * BK + load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        s_a[load_a_smem_k][load_a_smem_m] = r_load_a[<span class="number">0</span>];</span><br><span class="line">        s_a[load_a_smem_k + <span class="number">1</span>][load_a_smem_m] = r_load_a[<span class="number">1</span>];</span><br><span class="line">        s_a[load_a_smem_k + <span class="number">2</span>][load_a_smem_m] = r_load_a[<span class="number">2</span>];</span><br><span class="line">        s_a[load_a_smem_k + <span class="number">3</span>][load_a_smem_m] = r_load_a[<span class="number">3</span>];</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; ++k)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_a[k][ty * TM / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_a[k][ty * TM / <span class="number">2</span> + BM / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_b[k][tx * TN / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_b[k][tx * TN / <span class="number">2</span> + BN / <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> m = <span class="number">0</span>; m &lt; TM; ++m)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> n = <span class="number">0</span>; n &lt; TN; ++n) r_c[m][n] += r_comp_a[m] * r_comp_b[n];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM / <span class="number">2</span> + i;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM / <span class="number">2</span> + i + BM / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;计算结果如下,性能达到了理论峰值的68.6%，达到了实际可达峰值的80%：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">M N K =    <span class="number">128</span>    <span class="number">128</span>   <span class="number">1024</span>, Time =   <span class="number">0.00017306</span>   <span class="number">0.00017603</span>   <span class="number">0.00017715</span> s, AVG Performance =   <span class="number">190.6225</span> Gflops</span><br><span class="line">M N K =    <span class="number">192</span>    <span class="number">192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00014438</span>   <span class="number">0.00015247</span>   <span class="number">0.00016486</span> s, AVG Performance =   <span class="number">495.1511</span> Gflops</span><br><span class="line">M N K =    <span class="number">256</span>    <span class="number">256</span>   <span class="number">1024</span>, Time =   <span class="number">0.00014438</span>   <span class="number">0.00014479</span>   <span class="number">0.00014541</span> s, AVG Performance =   <span class="number">926.9590</span> Gflops</span><br><span class="line">M N K =    <span class="number">384</span>    <span class="number">384</span>   <span class="number">1024</span>, Time =   <span class="number">0.00014541</span>   <span class="number">0.00015677</span>   <span class="number">0.00016179</span> s, AVG Performance =  <span class="number">1926.3098</span> Gflops</span><br><span class="line">M N K =    <span class="number">512</span>    <span class="number">512</span>   <span class="number">1024</span>, Time =   <span class="number">0.00015258</span>   <span class="number">0.00015421</span>   <span class="number">0.00015872</span> s, AVG Performance =  <span class="number">3481.3280</span> Gflops</span><br><span class="line">M N K =    <span class="number">768</span>    <span class="number">768</span>   <span class="number">1024</span>, Time =   <span class="number">0.00015462</span>   <span class="number">0.00015954</span>   <span class="number">0.00016077</span> s, AVG Performance =  <span class="number">7571.5533</span> Gflops</span><br><span class="line">M N K =   <span class="number">1024</span>   <span class="number">1024</span>   <span class="number">1024</span>, Time =   <span class="number">0.00015770</span>   <span class="number">0.00016025</span>   <span class="number">0.00016278</span> s, AVG Performance = <span class="number">13400.6000</span> Gflops</span><br><span class="line">M N K =   <span class="number">1536</span>   <span class="number">1536</span>   <span class="number">1024</span>, Time =   <span class="number">0.00024166</span>   <span class="number">0.00024380</span>   <span class="number">0.00024678</span> s, AVG Performance = <span class="number">19819.2506</span> Gflops</span><br><span class="line">M N K =   <span class="number">2048</span>   <span class="number">2048</span>   <span class="number">1024</span>, Time =   <span class="number">0.00046899</span>   <span class="number">0.00047144</span>   <span class="number">0.00047411</span> s, AVG Performance = <span class="number">18220.6316</span> Gflops</span><br><span class="line">M N K =   <span class="number">3072</span>   <span class="number">3072</span>   <span class="number">1024</span>, Time =   <span class="number">0.00092467</span>   <span class="number">0.00093071</span>   <span class="number">0.00093798</span> s, AVG Performance = <span class="number">20766.1658</span> Gflops</span><br><span class="line">M N K =   <span class="number">4096</span>   <span class="number">4096</span>   <span class="number">1024</span>, Time =   <span class="number">0.00149402</span>   <span class="number">0.00150313</span>   <span class="number">0.00152064</span> s, AVG Performance = <span class="number">22858.7997</span> Gflops</span><br><span class="line">M N K =   <span class="number">6144</span>   <span class="number">6144</span>   <span class="number">1024</span>, Time =   <span class="number">0.00325222</span>   <span class="number">0.00327711</span>   <span class="number">0.00329728</span> s, AVG Performance = <span class="number">23590.7485</span> Gflops</span><br><span class="line">M N K =   <span class="number">8192</span>   <span class="number">8192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00568320</span>   <span class="number">0.00570696</span>   <span class="number">0.00574771</span> s, AVG Performance = <span class="number">24082.7044</span> Gflops</span><br><span class="line">M N K =  <span class="number">12288</span>  <span class="number">12288</span>   <span class="number">1024</span>, Time =   <span class="number">0.01249382</span>   <span class="number">0.01263268</span>   <span class="number">0.01291366</span> s, AVG Performance = <span class="number">24479.1828</span> Gflops</span><br><span class="line">M N K =  <span class="number">16384</span>  <span class="number">16384</span>   <span class="number">1024</span>, Time =   <span class="number">0.02226381</span>   <span class="number">0.02418770</span>   <span class="number">0.02583859</span> s, AVG Performance = <span class="number">22728.7352</span> Gflops</span><br></pre></td></tr></table></figure>
<h3 id="2-4-流水并行化：Double-Buffering（v3）"><a href="#2-4-流水并行化：Double-Buffering（v3）" class="headerlink" title="2.4 流水并行化：Double Buffering（v3）"></a>2.4 流水并行化：<strong>Double Buffering</strong>（v3）</h3><p>&emsp;&emsp;之前的方法存在 <strong>访存-计算</strong> 的串行模式流水线，这个方法就是提高访存和计算的并行程度，下图很形象，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/657632577">图片来源</a>。</p>
<p><img src="/2025/03/25/cuda-learning-03/image-20250508191629076.png" alt="image-20250508191629076"></p>
<p>&emsp;&emsp;具体到代码实现中，主要有一下几个点：</p>
<ul>
<li>需要原来两倍的Shared Memory</li>
<li>第一次加载数据在主循环之前，最后一次计算在主循环之后，主循环从 k = 1开始</li>
<li>由于计算和下一次访存使用的Shared Memory不同，因此主循环中每次循环只需要一次__syncthreads()</li>
<li>GPU不能像CPU那样支持乱序执行，主循环中需要先将下一次循环计算需要的数据从Gloabal Memory中load 到寄存器，然后进行本次计算，之后再将load到寄存器中的数据写到Shared Memory，这样在LDG指令向Global Memory做load时，不会影响后续FFMA及其它运算指令的 launch 执行，也就达到了Double Buffering的目的。</li>
</ul>
<p>&emsp;&emsp;关于第四个点，还是挺深奥的，<strong>涉及GPU指令的流水线化和异步内存访问</strong>。一开始，我在想，<strong>既然GPU不能乱序执行，那么不还是串行吗？实则不然</strong>。答案如下，懒得自己总结了。</p>
<p><img src="/2025/03/25/cuda-learning-03/image-20250508194642841.png" alt="image-20250508194642841"></p>
<p><img src="/2025/03/25/cuda-learning-03/image-20250508194700041.png" alt="image-20250508194700041"></p>
<p>还有一个问题就是<strong>LDG是异步的话，那么会不会存在数据竞争导致错误呢？</strong></p>
<p>&emsp;&emsp;答案是不会。硬件会自动插入<strong>依赖屏障</strong>，保证正确性。类似于CPU的<code>load</code>指令可以乱序执行，但编译器/硬件会保证<strong>数据依赖的正确性</strong>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 流水并行化：Double Buffering</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sgemm_V3</span><span class="params">(<span class="type">float</span> * __restrict__ a, <span class="type">float</span> * __restrict__ b, <span class="type">float</span> * __restrict__ c, <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * blockDim.x + tx;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_a[<span class="number">2</span>][BK][BM];</span><br><span class="line">    __shared__ <span class="type">float</span> s_b[<span class="number">2</span>][BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> r_c[TM][TN] = &#123;<span class="number">0.0</span>&#125;;</span><br><span class="line">    <span class="type">float</span> r_load_a[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_load_b[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_comp_a[TM];</span><br><span class="line">    <span class="type">float</span> r_comp_b[TN];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = tid &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">1</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k][load_a_smem_m] = r_load_a[<span class="number">0</span>];</span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k + <span class="number">1</span>][load_a_smem_m] = r_load_a[<span class="number">1</span>];</span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k + <span class="number">2</span>][load_a_smem_m] = r_load_a[<span class="number">2</span>];</span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k + <span class="number">3</span>][load_a_smem_m] = r_load_a[<span class="number">3</span>];</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[<span class="number">0</span>][load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> cnt = (K + BK - <span class="number">1</span>) / BK;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> bk = <span class="number">1</span>; bk &lt; cnt; ++bk)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> smem_sel = (bk - <span class="number">1</span>) &amp; <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> smem_sel_next = bk &amp; <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = bk * BK + load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = bk * BK + load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; ++k)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_a[smem_sel][k][ty * TM / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_a[smem_sel][k][ty * TM / <span class="number">2</span> + BM / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_b[smem_sel][k][tx * TN / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_b[smem_sel][k][tx * TN / <span class="number">2</span> + BN / <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> m = <span class="number">0</span>; m &lt; TM; ++m)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> n = <span class="number">0</span>; n &lt; TN; ++n) r_c[m][n] += r_comp_a[m] * r_comp_b[n];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k][load_a_smem_m] = r_load_a[<span class="number">0</span>];</span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k + <span class="number">1</span>][load_a_smem_m] = r_load_a[<span class="number">1</span>];</span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k + <span class="number">2</span>][load_a_smem_m] = r_load_a[<span class="number">2</span>];</span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k + <span class="number">3</span>][load_a_smem_m] = r_load_a[<span class="number">3</span>];</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[smem_sel_next][load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; ++k)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_a[<span class="number">1</span>][k][ty * TM / <span class="number">2</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_a[<span class="number">1</span>][k][ty * TM / <span class="number">2</span> + BM / <span class="number">2</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_b[<span class="number">1</span>][k][tx * TN / <span class="number">2</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_b[<span class="number">1</span>][k][tx * TN / <span class="number">2</span> + BN / <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> m = <span class="number">0</span>; m &lt; TM; ++m)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> n = <span class="number">0</span>; n &lt; TN; ++n) r_c[m][n] += r_comp_a[m] * r_comp_b[n];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM / <span class="number">2</span> + i;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM / <span class="number">2</span> + i + BM / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;计算结果如下,性能达到了理论峰值的75.7%，达到了实际可达峰值的90%：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">M N K =    <span class="number">128</span>    <span class="number">128</span>   <span class="number">1024</span>, Time =   <span class="number">0.00011162</span>   <span class="number">0.00011725</span>   <span class="number">0.00012186</span> s, AVG Performance =   <span class="number">286.1834</span> Gflops</span><br><span class="line">M N K =    <span class="number">192</span>    <span class="number">192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00011059</span>   <span class="number">0.00011192</span>   <span class="number">0.00011264</span> s, AVG Performance =   <span class="number">674.5471</span> Gflops</span><br><span class="line">M N K =    <span class="number">256</span>    <span class="number">256</span>   <span class="number">1024</span>, Time =   <span class="number">0.00011155</span>   <span class="number">0.00011243</span>   <span class="number">0.00011469</span> s, AVG Performance =  <span class="number">1193.8020</span> Gflops</span><br><span class="line">M N K =    <span class="number">384</span>    <span class="number">384</span>   <span class="number">1024</span>, Time =   <span class="number">0.00010957</span>   <span class="number">0.00011162</span>   <span class="number">0.00011366</span> s, AVG Performance =  <span class="number">2705.6147</span> Gflops</span><br><span class="line">M N K =    <span class="number">512</span>    <span class="number">512</span>   <span class="number">1024</span>, Time =   <span class="number">0.00010854</span>   <span class="number">0.00010957</span>   <span class="number">0.00011469</span> s, AVG Performance =  <span class="number">4899.8878</span> Gflops</span><br><span class="line">M N K =    <span class="number">768</span>    <span class="number">768</span>   <span class="number">1024</span>, Time =   <span class="number">0.00010957</span>   <span class="number">0.00011038</span>   <span class="number">0.00011162</span> s, AVG Performance = <span class="number">10943.2485</span> Gflops</span><br><span class="line">M N K =   <span class="number">1024</span>   <span class="number">1024</span>   <span class="number">1024</span>, Time =   <span class="number">0.00010957</span>   <span class="number">0.00011132</span>   <span class="number">0.00011776</span> s, AVG Performance = <span class="number">19291.3629</span> Gflops</span><br><span class="line">M N K =   <span class="number">1536</span>   <span class="number">1536</span>   <span class="number">1024</span>, Time =   <span class="number">0.00021299</span>   <span class="number">0.00021944</span>   <span class="number">0.00022221</span> s, AVG Performance = <span class="number">22019.2705</span> Gflops</span><br><span class="line">M N K =   <span class="number">2048</span>   <span class="number">2048</span>   <span class="number">1024</span>, Time =   <span class="number">0.00043715</span>   <span class="number">0.00044431</span>   <span class="number">0.00045056</span> s, AVG Performance = <span class="number">19333.1840</span> Gflops</span><br><span class="line">M N K =   <span class="number">3072</span>   <span class="number">3072</span>   <span class="number">1024</span>, Time =   <span class="number">0.00090624</span>   <span class="number">0.00091166</span>   <span class="number">0.00091648</span> s, AVG Performance = <span class="number">21200.2324</span> Gflops</span><br><span class="line">M N K =   <span class="number">4096</span>   <span class="number">4096</span>   <span class="number">1024</span>, Time =   <span class="number">0.00149094</span>   <span class="number">0.00151910</span>   <span class="number">0.00167629</span> s, AVG Performance = <span class="number">22618.4236</span> Gflops</span><br><span class="line">M N K =   <span class="number">6144</span>   <span class="number">6144</span>   <span class="number">1024</span>, Time =   <span class="number">0.00334234</span>   <span class="number">0.00342294</span>   <span class="number">0.00378880</span> s, AVG Performance = <span class="number">22585.6909</span> Gflops</span><br><span class="line">M N K =   <span class="number">8192</span>   <span class="number">8192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00587981</span>   <span class="number">0.00618916</span>   <span class="number">0.00649728</span> s, AVG Performance = <span class="number">22206.4044</span> Gflops</span><br><span class="line">M N K =  <span class="number">12288</span>  <span class="number">12288</span>   <span class="number">1024</span>, Time =   <span class="number">0.01135514</span>   <span class="number">0.01144668</span>   <span class="number">0.01189990</span> s, AVG Performance = <span class="number">27015.4839</span> Gflops</span><br><span class="line">M N K =  <span class="number">16384</span>  <span class="number">16384</span>   <span class="number">1024</span>, Time =   <span class="number">0.02017075</span>   <span class="number">0.02207642</span>   <span class="number">0.02387558</span> s, AVG Performance = <span class="number">24902.4032</span> Gflops</span><br></pre></td></tr></table></figure>
<h3 id="2-5-cuBLAS-性能"><a href="#2-5-cuBLAS-性能" class="headerlink" title="2.5 cuBLAS 性能"></a>2.5 cuBLAS 性能</h3><p>&emsp;&emsp;性能达到了理论峰值的83.9%，实际可达峰值的99%：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">M N K =    <span class="number">128</span>    <span class="number">128</span>   <span class="number">1024</span>, Time =   <span class="number">0.00001843</span>   <span class="number">0.00035505</span>   <span class="number">0.00335494</span> s, AVG Performance =    <span class="number">94.5063</span> Gflops</span><br><span class="line">M N K =    <span class="number">192</span>    <span class="number">192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00002253</span>   <span class="number">0.00026675</span>   <span class="number">0.00245965</span> s, AVG Performance =   <span class="number">283.0250</span> Gflops</span><br><span class="line">M N K =    <span class="number">256</span>    <span class="number">256</span>   <span class="number">1024</span>, Time =   <span class="number">0.00002662</span>   <span class="number">0.00003389</span>   <span class="number">0.00008397</span> s, AVG Performance =  <span class="number">3959.8791</span> Gflops</span><br><span class="line">M N K =    <span class="number">384</span>    <span class="number">384</span>   <span class="number">1024</span>, Time =   <span class="number">0.00003174</span>   <span class="number">0.00003840</span>   <span class="number">0.00005530</span> s, AVG Performance =  <span class="number">7864.9753</span> Gflops</span><br><span class="line">M N K =    <span class="number">512</span>    <span class="number">512</span>   <span class="number">1024</span>, Time =   <span class="number">0.00005120</span>   <span class="number">0.00005386</span>   <span class="number">0.00006554</span> s, AVG Performance =  <span class="number">9967.4525</span> Gflops</span><br><span class="line">M N K =    <span class="number">768</span>    <span class="number">768</span>   <span class="number">1024</span>, Time =   <span class="number">0.00008602</span>   <span class="number">0.00009175</span>   <span class="number">0.00013824</span> s, AVG Performance = <span class="number">13165.7144</span> Gflops</span><br><span class="line">M N K =   <span class="number">1024</span>   <span class="number">1024</span>   <span class="number">1024</span>, Time =   <span class="number">0.00012698</span>   <span class="number">0.00013035</span>   <span class="number">0.00014438</span> s, AVG Performance = <span class="number">16474.4968</span> Gflops</span><br><span class="line">M N K =   <span class="number">1536</span>   <span class="number">1536</span>   <span class="number">1024</span>, Time =   <span class="number">0.00022211</span>   <span class="number">0.00023521</span>   <span class="number">0.00034304</span> s, AVG Performance = <span class="number">20542.9712</span> Gflops</span><br><span class="line">M N K =   <span class="number">2048</span>   <span class="number">2048</span>   <span class="number">1024</span>, Time =   <span class="number">0.00042496</span>   <span class="number">0.00042792</span>   <span class="number">0.00044544</span> s, AVG Performance = <span class="number">20073.5427</span> Gflops</span><br><span class="line">M N K =   <span class="number">3072</span>   <span class="number">3072</span>   <span class="number">1024</span>, Time =   <span class="number">0.00084992</span>   <span class="number">0.00085535</span>   <span class="number">0.00088269</span> s, AVG Performance = <span class="number">22595.9154</span> Gflops</span><br><span class="line">M N K =   <span class="number">4096</span>   <span class="number">4096</span>   <span class="number">1024</span>, Time =   <span class="number">0.00146125</span>   <span class="number">0.00146872</span>   <span class="number">0.00149606</span> s, AVG Performance = <span class="number">23394.2915</span> Gflops</span><br><span class="line">M N K =   <span class="number">6144</span>   <span class="number">6144</span>   <span class="number">1024</span>, Time =   <span class="number">0.00306682</span>   <span class="number">0.00307189</span>   <span class="number">0.00308838</span> s, AVG Performance = <span class="number">25166.7152</span> Gflops</span><br><span class="line">M N K =   <span class="number">8192</span>   <span class="number">8192</span>   <span class="number">1024</span>, Time =   <span class="number">0.00457830</span>   <span class="number">0.00470508</span>   <span class="number">0.00521523</span> s, AVG Performance = <span class="number">29210.7880</span> Gflops</span><br><span class="line">M N K =  <span class="number">12288</span>  <span class="number">12288</span>   <span class="number">1024</span>, Time =   <span class="number">0.01022771</span>   <span class="number">0.01032509</span>   <span class="number">0.01057178</span> s, AVG Performance = <span class="number">29950.1034</span> Gflops</span><br><span class="line">M N K =  <span class="number">16384</span>  <span class="number">16384</span>   <span class="number">1024</span>, Time =   <span class="number">0.01822106</span>   <span class="number">0.01895342</span>   <span class="number">0.01967821</span> s, AVG Performance = <span class="number">29005.6252</span> Gflops</span><br></pre></td></tr></table></figure>
<h2 id="3-Hgemm"><a href="#3-Hgemm" class="headerlink" title="3. Hgemm"></a>3. Hgemm</h2><p>&emsp;&emsp;半精度浮点类型的矩阵乘法，使用tensor core。</p>
<h3 id="3-1-前置知识"><a href="#3-1-前置知识" class="headerlink" title="3.1 前置知识"></a>3.1 前置知识</h3><h4 id="3-1-1-cublas的调用"><a href="#3-1-1-cublas的调用" class="headerlink" title="3.1.1 cublas的调用"></a>3.1.1 cublas的调用</h4><p>cublas有个诡异的列优先原则，导致这个函数接口的传参不是那么容易，要好好注意这个是否转置、两个矩阵的顺序、主维等参数。给出两种方法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一种：诡异先b再a</span></span><br><span class="line"><span class="built_in">HGEMM_CHECK_CUBLAS_ERROR</span>(<span class="built_in">cublasGemmEx</span>(handle,</span><br><span class="line">                                      CUBLAS_OP_N,</span><br><span class="line">                                      CUBLAS_OP_N,</span><br><span class="line">                                      N, M, K,</span><br><span class="line">                                      &amp;alpha,</span><br><span class="line">                                      B, CUDA_R_16F, N,</span><br><span class="line">                                      A, CUDA_R_16F, K,</span><br><span class="line">                                      &amp;beta,</span><br><span class="line">                                      C, CUDA_R_16F, N,</span><br><span class="line">                                      CUBLAS_COMPUTE_16F,</span><br><span class="line">                                      CUBLAS_GEMM_DEFAULT_TENSOR_OP));</span><br></pre></td></tr></table></figure>
<p>第二种：喜欢转置说是<img src="/2025/03/25/cuda-learning-03/image-20250701011406779.png" alt="image-20250701011406779"></p>
<p>两种都能求的正确结果，但是明显方法1更好，因为不用转置，毕竟矩阵大了的话，转置需要的时间也不小。参考blog：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/cuancuancuanhao/p/7763256.html">有关CUBLAS中的矩阵乘法函数 - 爨爨爨好 - 博客园</a></p>
<h4 id="3-1-2-tensor-core-api"><a href="#3-1-2-tensor-core-api" class="headerlink" title="3.1.2 tensor core api"></a>3.1.2 tensor core api</h4><p>&emsp;&emsp;这里我们以<code>D = A * B + C</code> 为例。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 着重注意第一个参数和最后一个参数</span></span><br><span class="line"><span class="comment">// 作为A和B分别是wmma::matrix_a和wmma::matrix_b</span></span><br><span class="line"><span class="comment">// 用作源或目标累加器（C 或 D）时使用accumulator</span></span><br><span class="line"><span class="comment">// 对于 matrix_a 和 matrix_b 片段，必须指定 Layout 参数, 累加器不用填</span></span><br><span class="line">nvcuda::wmma::fragment&lt;wmma::matrix_a, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_a;</span><br><span class="line">nvcuda::wmma::fragment&lt;wmma::matrix_b, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_a;</span><br><span class="line">nvcuda::wmma::fragment&lt;wmma::accumulator, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half&gt; frag_c;</span><br><span class="line"></span><br><span class="line">nvcuda::wmma::<span class="built_in">fill_fragment</span>(frag_c, <span class="number">0.0</span>);</span><br><span class="line">nvcuda::wmma::<span class="built_in">load_matrix_sync</span>(frag_a, (shared memory <span class="keyword">or</span> global memory pointer), (stride_a));</span><br><span class="line">nvcuda::wmma::<span class="built_in">load_matrix_sync</span>(frag_b, (shared memory <span class="keyword">or</span> global memory pointer), (stride_b));</span><br><span class="line">nvcuda::wmma::<span class="built_in">mma_sync</span>(frag_c, frag_a, frag_b, frag_c);</span><br><span class="line">nvcuda::wmma::<span class="built_in">store_matrix_sync</span>((shared memory <span class="keyword">or</span> global memory pointer), frag_c, (stride_c), wmma::mem_row_major);</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/25/cuda-learning-03/image-20250701193517088.png" alt="image-20250701193517088"></p>
<h4 id="3-1-3-ptx-指令"><a href="#3-1-3-ptx-指令" class="headerlink" title="3.1.3 ptx 指令"></a>3.1.3 ptx 指令</h4><ul>
<li><p>ldmatrix</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> LDMATRIX_X1(R, addr) \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;ldmatrix.sync.aligned.x1.m8n8.shared.b16 &#123;%0&#125;, [%1];\n&quot;</span> : <span class="string">&quot;=r&quot;</span>(R) : <span class="string">&quot;r&quot;</span>(addr))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> LDMATRIX_X2(R0, R1, addr) \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;ldmatrix.sync.aligned.x2.m8n8.shared.b16 &#123;%0, %1&#125;, [%2];\n&quot;</span> : <span class="string">&quot;=r&quot;</span>(R0), <span class="string">&quot;=r&quot;</span>(R1) : <span class="string">&quot;r&quot;</span>(addr))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> LDMATRIX_X4(R0, R1, R2, R3, addr)                                             \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;ldmatrix.sync.aligned.x4.m8n8.shared.b16 &#123;%0, %1, %2, %3&#125;, [%4];\n&quot;</span> \</span></span><br><span class="line"><span class="meta">                 : <span class="string">&quot;=r&quot;</span>(R0), <span class="string">&quot;=r&quot;</span>(R1), <span class="string">&quot;=r&quot;</span>(R2), <span class="string">&quot;=r&quot;</span>(R3)                             \</span></span><br><span class="line"><span class="meta">                 : <span class="string">&quot;r&quot;</span>(addr))</span></span><br></pre></td></tr></table></figure>
<p>LDMATRIX_X1:加载一块8x8的半精度（b16）矩阵tile（通常为Tensor Core的数据载入方式）从shared memory 到单个寄存器（R）。</p>
<ul>
<li><code>ldmatrix</code>：矩阵块加载PTX指令</li>
<li><code>.sync.aligned</code>：同步、对齐方式</li>
<li><code>.x1</code>：一次加载1个tile（8x8）</li>
<li><code>.m8n8</code>：tile大小8x8</li>
<li><p><code>.shared.b16</code>：从shared memory以16位为单位加载(每 16 位为一个数据元素)</p>
</li>
<li><p><code>.x2</code>：每次加载2个tile</p>
</li>
<li><code>.x4</code>：每次加载4个tile</li>
</ul>
</li>
<li><p>HMMA16816</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> HMMA16816(RD0, RD1, RA0, RA1, RA2, RA3, RB0, RB1, RC0, RC1)                                                    \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 &#123;%0, %1&#125;, &#123;%2, %3, %4, %5&#125;, &#123;%6, %7&#125;, &#123;%8, %9&#125;;\n&quot;</span> \</span></span><br><span class="line"><span class="meta">                 : <span class="string">&quot;=r&quot;</span>(RD0), <span class="string">&quot;=r&quot;</span>(RD1)                                                                                \</span></span><br><span class="line"><span class="meta">                 : <span class="string">&quot;r&quot;</span>(RA0), <span class="string">&quot;r&quot;</span>(RA1), <span class="string">&quot;r&quot;</span>(RA2), <span class="string">&quot;r&quot;</span>(RA3), <span class="string">&quot;r&quot;</span>(RB0), <span class="string">&quot;r&quot;</span>(RB1), <span class="string">&quot;r&quot;</span>(RC0), <span class="string">&quot;r&quot;</span>(RC1))</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>mma</code>：矩阵-矩阵累加（Matrix Multiply Accumulate）PTX指令</li>
<li><code>.sync.aligned</code>：同步、对齐</li>
<li><code>.m16n8k16</code>：矩阵维度（MxN乘K）</li>
<li><code>.row.col</code>：内存布局（行主序/列主序）</li>
<li><code>f16.f16.f16.f16</code>：操作数和结果全为半精度float16</li>
<li>输出：<ul>
<li><code>&#123;%0, %1&#125;</code>：结果D的两个寄存器</li>
</ul>
</li>
<li>输入：<ul>
<li><code>RA0~RA3</code>、<code>RB0~RB1</code>：A和B矩阵块的寄存器</li>
<li><code>RC0, RC1</code>：累加用的C寄存器</li>
</ul>
</li>
</ul>
</li>
<li><p>CP_ASYNC</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> ((__CUDACC_VER_MAJOR__ == 11) &amp;&amp; (__CUDACC_VER_MINOR__ &gt;= 4)) || (__CUDACC_VER_MAJOR__ &gt; 11)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CP_ASYNC_CA(dst, src, Bytes) \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;cp.async.ca.shared.global.L2::128B [%0], [%1], %2;\n&quot;</span> ::<span class="string">&quot;r&quot;</span>(dst), <span class="string">&quot;l&quot;</span>(src), <span class="string">&quot;n&quot;</span>(Bytes))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CP_ASYNC_CG(dst, src, Bytes) \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\n&quot;</span> ::<span class="string">&quot;r&quot;</span>(dst), <span class="string">&quot;l&quot;</span>(src), <span class="string">&quot;n&quot;</span>(Bytes))</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CP_ASYNC_CA(dst, src, Bytes) \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], %2;\n&quot;</span> ::<span class="string">&quot;r&quot;</span>(dst), <span class="string">&quot;l&quot;</span>(src), <span class="string">&quot;n&quot;</span>(Bytes))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CP_ASYNC_CG(dst, src, Bytes) \</span></span><br><span class="line"><span class="meta">    asm volatile(<span class="string">&quot;cp.async.cg.shared.global [%0], [%1], %2;\n&quot;</span> ::<span class="string">&quot;r&quot;</span>(dst), <span class="string">&quot;l&quot;</span>(src), <span class="string">&quot;n&quot;</span>(Bytes))</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CP_ASYNC_COMMIT_GROUP() asm volatile(<span class="string">&quot;cp.async.commit_group;\n&quot;</span> ::)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CP_ASYNC_WAIT_GROUP(N) asm volatile(<span class="string">&quot;cp.async.wait_group %0;\n&quot;</span> ::<span class="string">&quot;n&quot;</span>(N))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CP_ASYNC_WAIT_ALL() asm volatile(<span class="string">&quot;cp.async.wait_all;\n&quot;</span> ::)</span></span><br></pre></td></tr></table></figure>
<p>CUDA 11.4 及以上:</p>
<ul>
<li><code>cp.async</code>：CUDA 8.0+ 新增的<strong>异步数据拷贝</strong>PTX指令（copy async），用于把全局内存数据异步搬到 shared memory。</li>
<li><code>ca</code>：cache all（全层缓存提示），告诉硬件这次搬运要经过所有缓存。</li>
<li><code>cg</code>：cache global（只缓存到 L1，跳过 L2）。</li>
<li><code>.L2::128B</code>：<strong>新语法</strong>，指明此次操作对 L2 cache 的控制方式（128 字节对齐/搬运），增加细粒度 cache 控制（11.4 新特性）。</li>
<li><code>shared.global</code>：从全局内存搬到共享内存。</li>
<li><code>[%0]</code>：目标 shared memory 地址（dst）。</li>
<li><code>[%1]</code>：源全局内存地址（src）。</li>
<li><code>%2</code>：要搬运的字节数（Bytes）。</li>
</ul>
<p>CUDA 11.3 及一下：<strong>只是没有 <code>.L2::128B</code> 后缀</strong>。因为11.4之前 PTX不支持L2 cache细粒度控</p>
<ul>
<li><code>cp.async.commit_group</code>：告诉GPU，在这之前的<code>cp.async</code>属于一组，并不是到这条命令的时候才会开始运行，之前就开始了。</li>
<li><code>cp.async.wait_group %0</code>：等待一个 已提交的异步拷贝操作组 完成。类似于 CUDA 的 __syncthreads()，但仅针对异步拷贝操作，而不是全部线程同步。</li>
</ul>
</li>
</ul>
<p>&emsp;&emsp;<del>关于这些api 和 指令，不如直接看下面有例子，看例子更好理解。</del></p>
<h3 id="3-2-v1"><a href="#3-2-v1" class="headerlink" title="3.2 v1"></a>3.2 v1</h3><p>&emsp;&emsp;首先讨论BM等数据的选取，具体数值来源于其他博客的分析。</p>
<p>&emsp;&emsp;BM和BN是越大越好，然后打算选取<code>16*16*16</code>的的tensor core，BK至少需要是<code>nvcuda::wmma::fragment</code>中定义矩阵的K维度的整数倍；当BK太小（例如取BK = 16）时，核心循环中HMMA指令占比不高，一些循环相关的地址计算的指令会导致性能下降；当BK &gt;= 32时，发现性能基本不会再随BK而提高了；加之hared memory、Registers的限制，最终取<code>BM = 128，BN = 256，BK = 32，thread_per_block = 256</code>。</p>
<p>&emsp;&emsp;这样每次K循环中，256个线程每个线程需要取16个矩阵A的元素，取32个矩阵B的元素；8个warp每个warp负责计算64x32x64的矩阵乘法。为了方便起见假设M/N/K对齐到128/256/32，也就是没有处理corner case。</p>
<p>&emsp;&emsp;调用的C++ wmma的API，代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myHGEMMAlignedV1</span><span class="params">(half * __restrict__ a, half * __restrict__ b, half * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">256</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> wid = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> APAD = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BPAD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">    __shared__ half s_a[BM][BK + APAD];</span><br><span class="line">    __shared__ half s_b[BK][BN + BPAD];</span><br><span class="line"></span><br><span class="line">    wmma::fragment&lt;wmma::matrix_a, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_a[<span class="number">2</span>][<span class="number">4</span>];</span><br><span class="line">    wmma::fragment&lt;wmma::matrix_b, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_b[<span class="number">2</span>][<span class="number">4</span>];</span><br><span class="line">    wmma::fragment&lt;wmma::accumulator, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half&gt; frag_c[<span class="number">4</span>][<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            wmma::<span class="built_in">fill_fragment</span>(frag_c[i][j], <span class="number">0.0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = (tid &gt;&gt; <span class="number">2</span>) &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">3</span>) &lt;&lt; <span class="number">3</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = (tid &gt;&gt; <span class="number">5</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_smem_k, K);</span><br><span class="line">    <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_smem_k, load_b_gmem_n, N);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> comp_c_frag_m = wid &amp; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> comp_c_frag_n = wid &gt;&gt; <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> bk = <span class="number">0</span>; bk &lt; K / BK; ++bk)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_a[load_a_smem_m][load_a_smem_k]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_a[load_a_smem_m + <span class="number">1</span>][load_a_smem_k]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr + K]);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k + <span class="number">1</span>][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr + N]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k + <span class="number">2</span>][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr + <span class="number">2</span> * N]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k + <span class="number">3</span>][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr + <span class="number">3</span> * N]);</span><br><span class="line">        </span><br><span class="line">        load_a_gmem_addr += BK;</span><br><span class="line">        load_b_gmem_addr += BK * N;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取的一个16*16的块，整个warp协作取</span></span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">0</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">1</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">2</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">3</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">0</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">1</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">2</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">3</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line"></span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">0</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">1</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">2</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">3</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">0</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">1</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">2</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">3</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line">    </span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">0</span>][i], frag_b[<span class="number">0</span>][j], frag_c[i][j]);</span><br><span class="line">                wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">1</span>][i], frag_b[<span class="number">1</span>][j], frag_c[i][j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> store_c_gmem_m = by * BM + comp_c_frag_m * <span class="number">64</span>;</span><br><span class="line">    <span class="type">int</span> store_c_gmem_n = bx * BN + comp_c_frag_n * <span class="number">64</span>;</span><br><span class="line">    <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            wmma::<span class="built_in">store_matrix_sync</span>(&amp;c[store_c_gmem_addr + i * <span class="number">16</span> * N + j * <span class="number">16</span>], frag_c[i][j], N, wmma::mem_row_major);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里有三个疑问，一是<strong>每个线程取16个矩阵A的元素，目前是每个线程取相邻两行的连续8列，而不是取一行的16列。关于为什么要这样？</strong></p>
<p>&emsp;&emsp;没找到准确的回答，大体上是<strong>有利于合并访存、方便后续共享内存布局、WMMA tile加载、更好地支持多warp并行，避免共享内存bank conflict。</strong></p>
<p>&emsp;&emsp;二是<strong>关于frag_a的填充，发现frag_a的列坐标在增加，而对应读取的s_a是行坐标在+16，这样写是否是出于优化的角度？是否将frag_a的定义改成<code>frag[4][2]</code>更符合语义？</strong></p>
<p>&emsp;&emsp;<strong>并不是出于优化的角度，只是通常的代码习惯。</strong>主流的wmma相关代码习惯是BK方向在前（方便做K方向累加），M/N方向在后，并且方便后面<code>wmma::mma_sync</code>的调用。</p>
<p>&emsp;&emsp;三是<strong>关于bank confilct的解决，这里是通过加了16 Bytes的pad解决的，为什么能解决？</strong></p>
<p>&emsp;&emsp;关于这个，我并没有想明白，这部分对smem的读取是warp协作读取的，我不会确定每个线程读取哪些数据，但是它应该是确实能避免的，并且这似乎也是一种官方常用方式。</p>
<h3 id="3-3-Hgemm-v2-Global-Memory到Shared-Memory的异步拷贝"><a href="#3-3-Hgemm-v2-Global-Memory到Shared-Memory的异步拷贝" class="headerlink" title="3.3 Hgemm v2: Global Memory到Shared Memory的异步拷贝"></a>3.3 Hgemm v2: Global Memory到Shared Memory的异步拷贝</h3><p>&emsp;&emsp;对全局内存的访问通过异步拷贝实现，即利用前面提到的<code>cp.async</code>指令，注意smem的首地址要用<code>__cvta_generic_to_shared()</code>获取，具体看代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myHGEMMAlignedV2</span><span class="params">(half * __restrict__ a, half * __restrict__ b, half * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">256</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> wid = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> APAD = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BPAD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">    __shared__ half s_a[BM][BK + APAD];</span><br><span class="line">    __shared__ half s_b[BK][BN + BPAD];</span><br><span class="line"></span><br><span class="line">    wmma::fragment&lt;wmma::matrix_a, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_a[<span class="number">2</span>][<span class="number">4</span>];</span><br><span class="line">    wmma::fragment&lt;wmma::matrix_b, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_b[<span class="number">2</span>][<span class="number">4</span>];</span><br><span class="line">    wmma::fragment&lt;wmma::accumulator, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half&gt; frag_c[<span class="number">4</span>][<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            wmma::<span class="built_in">fill_fragment</span>(frag_c[i][j], <span class="number">0.0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = (tid &gt;&gt; <span class="number">2</span>) &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">3</span>) &lt;&lt; <span class="number">3</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = (tid &gt;&gt; <span class="number">5</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> s_a_base_addr = __cvta_generic_to_shared(s_a[<span class="number">0</span>]);</span><br><span class="line">    <span class="type">int</span> s_b_base_addr = __cvta_generic_to_shared(s_b[<span class="number">0</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> load_a_smem_addr_0 = s_a_base_addr + <span class="built_in">OFFSET</span>(load_a_smem_m, load_a_smem_k, BK + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_a_smem_addr_1 = load_a_smem_addr_0 + (BK + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_0 = s_b_base_addr + <span class="built_in">OFFSET</span>(load_b_smem_k, load_b_smem_n, BN + BPAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_1 = load_b_smem_addr_0 + (BN + BPAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_2 = load_b_smem_addr_0 + <span class="number">2</span> * (BN + BPAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_3 = load_b_smem_addr_0 + <span class="number">3</span> * (BN + BPAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_smem_k, K);</span><br><span class="line">    <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_smem_k, load_b_gmem_n, N);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> comp_c_frag_m = wid &amp; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> comp_c_frag_n = wid &gt;&gt; <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> bk = <span class="number">0</span>; bk &lt; K / BK; ++bk)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">            : <span class="string">&quot;r&quot;</span>(load_a_smem_addr_0), <span class="string">&quot;l&quot;</span>(&amp;a[load_a_gmem_addr]));</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">            : <span class="string">&quot;r&quot;</span>(load_a_smem_addr_1), <span class="string">&quot;l&quot;</span>(&amp;a[load_a_gmem_addr + K]));</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">            : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_0), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr]));        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">            : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_1), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + N]));</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">            : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_2), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + <span class="number">2</span> * N]));        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">            : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_3), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + <span class="number">3</span> * N]));</span><br><span class="line">            </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.commit_group;\n&quot;</span> ::);</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.wait_group 0;\n&quot;</span> ::);</span><br><span class="line"></span><br><span class="line">        load_a_gmem_addr += BK;</span><br><span class="line">        load_b_gmem_addr += BK * N;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">0</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">1</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">2</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">3</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>][<span class="number">0</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">0</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">1</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">2</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">3</span>], &amp;s_a[comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>][<span class="number">16</span>], BK + APAD);</span><br><span class="line"></span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">0</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">1</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">2</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">3</span>], &amp;s_b[<span class="number">0</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">0</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">1</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">2</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">3</span>], &amp;s_b[<span class="number">16</span>][comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">0</span>][i], frag_b[<span class="number">0</span>][j], frag_c[i][j]);</span><br><span class="line">                wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">1</span>][i], frag_b[<span class="number">1</span>][j], frag_c[i][j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> store_c_gmem_m = by * BM + comp_c_frag_m * <span class="number">64</span>;</span><br><span class="line">    <span class="type">int</span> store_c_gmem_n = bx * BN + comp_c_frag_n * <span class="number">64</span>;</span><br><span class="line">    <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            wmma::<span class="built_in">store_matrix_sync</span>(&amp;c[store_c_gmem_addr + i * <span class="number">16</span> * N + j * <span class="number">16</span>], frag_c[i][j], N, wmma::mem_row_major);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;</p>
<h3 id="3-4-v2-Double-Buffer"><a href="#3-4-v2-Double-Buffer" class="headerlink" title="3.4 v2: Double Buffer"></a>3.4 v2: Double Buffer</h3><p>&emsp;&emsp;Sgemm中详细介绍过，不多废话了，需要注意的是double buffer会用到两倍的shared memory，当使用的shared memory超过48 KB时，需要使用dynamic shared memory。kernel的怕配置和调用方式如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>, BN = <span class="number">256</span>, BK = <span class="number">32</span>;</span><br><span class="line"><span class="function">dim3 <span class="title">blockDim</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line"><span class="type">int</span> BX = (N + BN - <span class="number">1</span>) / BN;</span><br><span class="line"><span class="type">int</span> BY = (M + BM - <span class="number">1</span>) / BM;</span><br><span class="line"><span class="function">dim3 <span class="title">gridDim</span><span class="params">(BX, BY)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaFuncSetAttribute</span>(myHGEMMAlignedV3, cudaFuncAttributeMaxDynamicSharedMemorySize, <span class="number">98304</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> dsmem = <span class="number">2</span> * (BM * (BK + <span class="number">8</span>) + BK * (BN + <span class="number">8</span>)) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">myHGEMMAlignedV3&lt;&lt;&lt;gridDim, blockDim, dsmem&gt;&gt;&gt;(a, b, c, M, N, K);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myHGEMMAlignedV3</span><span class="params">(half * __restrict__ a, half * __restrict__ b, half * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">256</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> wid = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> APAD = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BPAD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">extern</span> __shared__ half smem[];</span><br><span class="line">    half *s_a = smem;</span><br><span class="line">    half *s_b = smem + <span class="number">2</span> * BM * (BK + APAD);</span><br><span class="line">    <span class="type">int</span> s_a_db_offset = BM * (BK + APAD);</span><br><span class="line">    <span class="type">int</span> s_b_db_offset = BK * (BN + BPAD);</span><br><span class="line"></span><br><span class="line">    wmma::fragment&lt;wmma::matrix_a, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_a[<span class="number">2</span>][<span class="number">4</span>];</span><br><span class="line">    wmma::fragment&lt;wmma::matrix_b, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; frag_b[<span class="number">2</span>][<span class="number">4</span>];</span><br><span class="line">    wmma::fragment&lt;wmma::accumulator, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half&gt; frag_c[<span class="number">4</span>][<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            wmma::<span class="built_in">fill_fragment</span>(frag_c[i][j], <span class="number">0.0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = (tid &gt;&gt; <span class="number">2</span>) &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">3</span>) &lt;&lt; <span class="number">3</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = (tid &gt;&gt; <span class="number">5</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> s_a_base_addr = __cvta_generic_to_shared(s_a);</span><br><span class="line">    <span class="type">int</span> s_b_base_addr = __cvta_generic_to_shared(s_b);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_addr_0 = s_a_base_addr + <span class="built_in">OFFSET</span>(load_a_smem_m, load_a_smem_k, BK + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_a_smem_addr_1 = load_a_smem_addr_0 + (BK + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_0 = s_b_base_addr + <span class="built_in">OFFSET</span>(load_b_smem_k, load_b_smem_n, BN + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_1 = load_b_smem_addr_0 + (BN + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_2 = load_b_smem_addr_0 + <span class="number">2</span> * (BN + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">    <span class="type">int</span> load_b_smem_addr_3 = load_b_smem_addr_0 + <span class="number">3</span> * (BN + APAD) * <span class="built_in">sizeof</span>(half);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_smem_k, K);</span><br><span class="line">    <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_smem_k, load_b_gmem_n, N);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> comp_c_frag_m = wid &amp; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> comp_c_frag_n = wid &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_a_smem_addr_0), <span class="string">&quot;l&quot;</span>(&amp;a[load_a_gmem_addr]));</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_a_smem_addr_1), <span class="string">&quot;l&quot;</span>(&amp;a[load_a_gmem_addr + K]));</span><br><span class="line"></span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_0), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr]));        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_1), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + N]));</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_2), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + <span class="number">2</span> * N]));        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_3), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + <span class="number">3</span> * N]));</span><br><span class="line"></span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.commit_group;\n&quot;</span> ::);</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.wait_group 0;\n&quot;</span> ::);</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> smem_sel, smem_sel_next;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> bk = <span class="number">1</span>; bk &lt; K / BK; ++bk)</span><br><span class="line">    &#123;</span><br><span class="line">        smem_sel = (bk &amp; <span class="number">1</span>) ^ <span class="number">1</span>;</span><br><span class="line">        smem_sel_next = ((bk - <span class="number">1</span>) &amp; <span class="number">1</span>) ^ <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        load_a_gmem_addr += BK;</span><br><span class="line">        load_b_gmem_addr += BK * N;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_a_smem_addr_0 + smem_sel_next * s_a_db_offset * (<span class="type">int</span>)<span class="built_in">sizeof</span>(half)), <span class="string">&quot;l&quot;</span>(&amp;a[load_a_gmem_addr]));</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_a_smem_addr_1 + smem_sel_next * s_a_db_offset * (<span class="type">int</span>)<span class="built_in">sizeof</span>(half)), <span class="string">&quot;l&quot;</span>(&amp;a[load_a_gmem_addr + K]));</span><br><span class="line"></span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_0 + smem_sel_next * s_b_db_offset * (<span class="type">int</span>)<span class="built_in">sizeof</span>(half)), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr]));        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_1 + smem_sel_next * s_b_db_offset * (<span class="type">int</span>)<span class="built_in">sizeof</span>(half)), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + N]));</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_2 + smem_sel_next * s_b_db_offset * (<span class="type">int</span>)<span class="built_in">sizeof</span>(half)), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + <span class="number">2</span> * N]));        </span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.ca.shared.global [%0], [%1], 16;\n&quot;</span> :</span><br><span class="line">        : <span class="string">&quot;r&quot;</span>(load_b_smem_addr_3 + smem_sel_next * s_b_db_offset * (<span class="type">int</span>)<span class="built_in">sizeof</span>(half)), <span class="string">&quot;l&quot;</span>(&amp;b[load_b_gmem_addr + <span class="number">3</span> * N]));</span><br><span class="line"></span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">0</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">1</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">2</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">3</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">0</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">1</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">2</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">3</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line"></span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">0</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">1</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">2</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">3</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">0</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">1</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">2</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">        wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">3</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">0</span>][i], frag_b[<span class="number">0</span>][j], frag_c[i][j]);</span><br><span class="line">                wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">1</span>][i], frag_b[<span class="number">1</span>][j], frag_c[i][j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.commit_group;\n&quot;</span> ::);</span><br><span class="line">        <span class="built_in">asm</span> (<span class="string">&quot;cp.async.wait_group 0;\n&quot;</span> ::);</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    smem_sel = ((K / BK) &amp; <span class="number">1</span>) ^ <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">0</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">1</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">2</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">0</span>][<span class="number">3</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>) * (BK + APAD)], BK + APAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">0</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">1</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">16</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">2</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">32</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_a[<span class="number">1</span>][<span class="number">3</span>], &amp;s_a[smem_sel * s_a_db_offset + (comp_c_frag_m * <span class="number">64</span> + <span class="number">48</span>) * (BK + APAD) + <span class="number">16</span>], BK + APAD);</span><br><span class="line"></span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">0</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">1</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">2</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">0</span>][<span class="number">3</span>], &amp;s_b[smem_sel * s_b_db_offset + comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">0</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span>], BN + BPAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">1</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span> + <span class="number">16</span>], BN + BPAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">2</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span> + <span class="number">32</span>], BN + BPAD);</span><br><span class="line">    wmma::<span class="built_in">load_matrix_sync</span>(frag_b[<span class="number">1</span>][<span class="number">3</span>], &amp;s_b[smem_sel * s_b_db_offset + <span class="number">16</span> * (BN + BPAD) + comp_c_frag_n * <span class="number">64</span> + <span class="number">48</span>], BN + BPAD);</span><br><span class="line">    </span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">0</span>][i], frag_b[<span class="number">0</span>][j], frag_c[i][j]);</span><br><span class="line">            wmma::<span class="built_in">mma_sync</span>(frag_c[i][j], frag_a[<span class="number">1</span>][i], frag_b[<span class="number">1</span>][j], frag_c[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> store_c_gmem_m = by * BM + comp_c_frag_m * <span class="number">64</span>;</span><br><span class="line">    <span class="type">int</span> store_c_gmem_n = bx * BN + comp_c_frag_n * <span class="number">64</span>;</span><br><span class="line">    <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            wmma::<span class="built_in">store_matrix_sync</span>(&amp;c[store_c_gmem_addr + i * <span class="number">16</span> * N + j * <span class="number">16</span>], frag_c[i][j], N, wmma::mem_row_major);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="3-5-v4-提高L2-Cache的局部性"><a href="#3-5-v4-提高L2-Cache的局部性" class="headerlink" title="3.5 v4: 提高L2 Cache的局部性"></a>3.5 v4: 提高L2 Cache的局部性</h3><p>&emsp;&emsp;RTX3090一共有82个SM，经过计算v3的优化，每个SM只能容纳一个block，当大规模矩阵乘法的block数目超过82时，会按照gridDim.z -&gt; gridDim.y -&gt; gridDim.x这样的循环顺序进行调度。</p>
<p>&emsp;&emsp;例如当M = N = K = 16384时，矩阵C会被分块成128 * 64个Tile，如果按照正常的调度顺序，先调度矩阵C第一行64个Tile对应的block加上第二行的前18个block，这样虽然矩阵A的局部性很好，但是矩阵B的访存局部性极差。所以考虑平衡矩阵A和矩阵B的局部性，现在改成第一次先调度第一行到第五行的前16个block，加上第六行的前2个block。</p>
<p>&emsp;&emsp;主要需要做的是修改一下调用kernel时的代码，利用其默认的调度顺序，加上gridDim.z这一维，这里NSPLIT就代表矩阵C的一行一次调度NSPLIT列就改转到下一行（NSPLIT = 16 * 256 = 4096）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>, BN = <span class="number">256</span>, BK = <span class="number">32</span>;</span><br><span class="line"><span class="function">dim3 <span class="title">blockDim</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line"><span class="type">int</span> BX = (N + BN - <span class="number">1</span>) / BN;</span><br><span class="line"><span class="type">int</span> BY = (M + BM - <span class="number">1</span>) / BM;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> NSPLIT = <span class="number">4096</span>;</span><br><span class="line"><span class="type">int</span> split_num = (N + NSPLIT - <span class="number">1</span>) / NSPLIT;</span><br><span class="line"><span class="function">dim3 <span class="title">gridDim</span><span class="params">((BX + split_num - <span class="number">1</span>) / split_num, BY, split_num)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaFuncSetAttribute</span>(myHGEMMAlignedV4, cudaFuncAttributeMaxDynamicSharedMemorySize, <span class="number">98304</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> dsmem = <span class="number">2</span> * (BM * (BK + <span class="number">8</span>) + BK * (BN + <span class="number">8</span>)) * <span class="built_in">sizeof</span>(half);</span><br><span class="line">myHGEMMAlignedV4&lt;&lt;&lt;gridDim, blockDim, dsmem&gt;&gt;&gt;(a, b, c, M, N, K);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myHGEMMAlignedV4</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    half * __restrict__ a, half * __restrict__ b, half * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// int bx = blockIdx.x; // 原来是这样</span></span><br><span class="line">    <span class="type">int</span> bx = blockIdx.z * gridDim.x + blockIdx.x; <span class="comment">// 现在是这样</span></span><br><span class="line">    <span class="keyword">if</span> (bx &gt;= N / BN || by &gt;= M / BM)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-6-v5-循环展开"><a href="#3-6-v5-循环展开" class="headerlink" title="3.6 v5: 循环展开"></a>3.6 v5: 循环展开</h3><p>&emsp;&emsp;主要修改是显式要求编译器对循环完全展开32次（如果 <code>K/BK</code> 比32大，则只会unroll前32次），没有显式的 <code>#pragma unroll</code>，即让编译器默认决定是否展开循环。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myHGEMMAlignedV5</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    half * __restrict__ a, half * __restrict__ b, half * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll 32</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> bk = <span class="number">1</span>; bk &lt; K / BK; bk++) &#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;关于hgemm这部分的性能，可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/555339335">这篇文章</a>的情况，实测和文中相差不大，v5最后最好的情况下达到了136TFLOPS。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/703256080">从啥也不会到CUDA GEMM优化</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/657632577">CUDA（三）：通用矩阵乘法：从入门到熟练</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/ifromeast/cuda_learning/tree/main/03_gemm">https://github.com/ifromeast/cuda_learning/tree/main/03_gemm</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442930482">深入浅出GPU优化系列：GEMM优化（二）</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/441146275">[施工中] CUDA GEMM 理论性能分析与 kernel 优化</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/518857175">CUDA SGEMM矩阵乘法优化笔记——从入门到cublas</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/555339335">CUDA Ampere Tensor Core HGEMM 矩阵乘法优化笔记 —— Up To 131 TFLOPS!</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/nicolaswilde/cuda-tensorcore-hgemm">https://github.com/nicolaswilde/cuda-tensorcore-hgemm</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://space.bilibili.com/218427631/video">https://space.bilibili.com/218427631/video</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/cuancuancuanhao/p/7763256.html">有关CUBLAS中的矩阵乘法函数 - 爨爨爨好 - 博客园</a></p>
</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CUDA%E7%BC%96%E7%A8%8B/" rel="tag"># CUDA编程</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/12/29/OpenCL/" rel="prev" title="OpenCL 入门">
                  <i class="fa fa-angle-left"></i> OpenCL 入门
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Asuka</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
